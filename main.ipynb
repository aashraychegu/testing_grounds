{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0GEJLtggOUms",
    "outputId": "6b9e1d8e-f221-4914-ef7e-1971eddc823e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file generated_images already exists.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from modelsxn import *\n",
    "from PIL import Image\n",
    "!mkdir generated_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Xq4beSAoQSDA"
   },
   "outputs": [],
   "source": [
    "lr_gen = 0.0006*2 #Learning rate for generator\n",
    "lr_dis = 0.0003*2 #Learning rate for discriminator\n",
    "latent_dim = 256 #Latent dimension\n",
    "epoch = 200 #Number of epoch\n",
    "weight_decay = 1e-3 #Weight decay\n",
    "drop_rate = 0.5 #dropout\n",
    "\n",
    "# architecture details by authors\n",
    "image_size = 128 #H,W size of image for discriminator\n",
    "initial_size = 8 #Initial size for generator\n",
    "patch_size = 4 #Patch size for generated image\n",
    "num_classes = 1 #Number of classes for discriminator \n",
    "output_dir = 'checkpoint' #saved model path\n",
    "dim = 384 #Embedding dimension\n",
    "phi = 2 #\n",
    "beta1 = 0.9 #\n",
    "beta2 = 0.99 #\n",
    "diff_aug = \"translation,cutout,color\" #data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cS5Oxb7WcAts",
    "outputId": "90caa77c-f93d-48c1-cc46-1e24c5357d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 32\n",
      "2 4\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "generator = (\n",
    "    Generator(\n",
    "        depth1=6,\n",
    "        depth2=4,\n",
    "        depth3=1,\n",
    "        start_image_side_size=4,\n",
    "        dim=128*2,\n",
    "        heads=4,\n",
    "        mlp_ratio=4,\n",
    "        drop_rate=0.5,\n",
    "        latent_dim=256,\n",
    "        psfac=4,\n",
    "        cvupmult = 2\n",
    "    ).to(device).apply(inits_weight)\n",
    ")\n",
    "\n",
    "discriminator = (\n",
    "    Discriminator(\n",
    "        diff_aug=diff_aug,\n",
    "        image_size=64,\n",
    "        patch_size=4,\n",
    "        input_channel=1,\n",
    "        num_classes=1,\n",
    "        dim=384,\n",
    "        depth=3,\n",
    "        heads=4,\n",
    "        mlp_ratio=2,\n",
    "        drop_rate=0.5,\n",
    "    )\n",
    "    .to(device)\n",
    "    .apply(inits_weight)\n",
    ")\n",
    "\n",
    "optim_gen = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "    lr=lr_gen,\n",
    "    betas=(beta1, beta2),\n",
    ")\n",
    "optim_dis = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, discriminator.parameters()),\n",
    "    lr=lr_dis,\n",
    "    betas=(beta1, beta2),\n",
    ")\n",
    "\n",
    "gen_scheduler = optim.lr_scheduler.LinearLR(optim_gen, start_factor = 1, end_factor = .5, total_iters = 2000*200)\n",
    "dis_scheduler = optim.lr_scheduler.LinearLR(optim_gen, start_factor = 1, end_factor = .5, total_iters = 2000*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples, phi):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1))).to(\n",
    "        real_samples.get_device()\n",
    "    )\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(\n",
    "        True\n",
    "    )\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones([real_samples.shape[0], 1], requires_grad=False).to(\n",
    "        real_samples.get_device()\n",
    "    )\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.contiguous().view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - phi) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oqrgmomNf01n"
   },
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Grayscale(1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_set = torchvision.datasets.ImageFolder(\n",
    "        root=r\"C:\\Users\\aashr\\Desktop\\research\\testing_grounds\\images\",\n",
    "        transform=transform,\n",
    "    )\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=8, shuffle=True, drop_last = True\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    noise,\n",
    "    generator,\n",
    "    discriminator,\n",
    "    optim_gen,\n",
    "    optim_dis,\n",
    "    epoch,\n",
    "    gen_scheduler,\n",
    "    dis_scheduler,\n",
    "    latent_dim=latent_dim,\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    generator = generator.train()\n",
    "    discriminator = discriminator.train()\n",
    "\n",
    "    for index, (img, _) in enumerate(train_loader):\n",
    "\n",
    "        real_imgs = img.type(torch.cuda.FloatTensor)\n",
    "        noise = torch.cuda.FloatTensor(\n",
    "            np.random.normal(0, 1, (img.shape[0], latent_dim))\n",
    "        )\n",
    "\n",
    "        optim_dis.zero_grad()\n",
    "        real_valid = discriminator(real_imgs)\n",
    "        fake_imgs = generator(noise).detach()\n",
    "        fake_valid = discriminator(fake_imgs)\n",
    "\n",
    "        gradient_penalty = compute_gradient_penalty(\n",
    "            discriminator, real_imgs, fake_imgs.detach(), phi\n",
    "        )\n",
    "        disc_loss = (\n",
    "            -torch.mean(real_valid)\n",
    "            + torch.mean(fake_valid)\n",
    "            + gradient_penalty * 10 / (phi**2)\n",
    "        )\n",
    "        \n",
    "        disc_loss.backward()\n",
    "        optim_dis.step()\n",
    "        dis_scheduler.step()\n",
    "        d_lr = dis_scheduler.get_last_lr()\n",
    "\n",
    "\n",
    "        optim_gen.zero_grad()\n",
    "        gener_noise = torch.cuda.FloatTensor(np.random.normal(0, 1, (img.shape[0], latent_dim)))\n",
    "        print(noise.shape)\n",
    "        generated_imgs = generator(gener_noise)\n",
    "        fake_valid = discriminator(generated_imgs)\n",
    "        gener_loss = -torch.mean(fake_valid).to(device)\n",
    "        gener_loss.backward()\n",
    "        \n",
    "        optim_gen.step()\n",
    "        gen_scheduler.step()\n",
    "        g_lr = gen_scheduler.get_last_lr()\n",
    "\n",
    "        print(f\"\\r[Epoch {epoch+1}] [Batch {index+1}/{len(train_loader)}] [D loss: {disc_loss.item()}] [G loss: {gener_loss.item()}] [D lr: {d_lr[0]}] [G lr: {g_lr[0]}] {img.shape[0], generated_imgs.shape}                \\r\",end = \"\\r\")\n",
    "        del fake_valid, real_valid, fake_imgs, real_imgs, disc_loss, gener_loss, gradient_penalty\n",
    "        \n",
    "    save_image(\n",
    "        generated_imgs,\n",
    "        f\"generated_images/generated_img_{epoch}_{img_size}.png\",\n",
    "    )\n",
    "    display(\n",
    "        Image.open(\n",
    "            f\"generated_images/generated_img_{epoch}_{img_size}.png\"\n",
    "        )\n",
    "    )\n",
    "    del generated_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvwXcTahgPAV",
    "outputId": "b92e59f3-23d2-42f1-cf40-c7cb1f2e42e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashr\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3436.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 256]) initial MLP\n",
      "torch.Size([8, 16, 256]) encoder block 1\n",
      "torch.Size([8, 256, 16]) upsampling 1\n",
      "torch.Size([8, 256, 32]) cvup 1\n",
      "torch.Size([8, 256, 32]) encoder block 2\n",
      "torch.Size([8, 4096, 2]) upsampling 2\n",
      "torch.Size([8, 4096, 4]) cvup 2\n",
      "torch.Size([8, 4096, 4]) encoder block 3\n",
      "torch.Size([8, 1, 64, 64]) reshaping for output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashr\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n",
      "torch.Size([8, 16, 256]) initial MLP\n",
      "torch.Size([8, 16, 256]) encoder block 1\n",
      "torch.Size([8, 256, 16]) upsampling 1\n",
      "torch.Size([8, 256, 32]) cvup 1\n",
      "torch.Size([8, 256, 32]) encoder block 2\n",
      "torch.Size([8, 4096, 2]) upsampling 2\n",
      "torch.Size([8, 4096, 4]) cvup 2\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 8.00 GiB total capacity; 4.69 GiB already allocated; 201.02 MiB free; 5.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m2000\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[1;32m----> 3\u001b[0m     train(\n\u001b[0;32m      4\u001b[0m         noise,\n\u001b[0;32m      5\u001b[0m         generator,\n\u001b[0;32m      6\u001b[0m         discriminator,\n\u001b[0;32m      7\u001b[0m         optim_gen,\n\u001b[0;32m      8\u001b[0m         optim_dis,\n\u001b[0;32m      9\u001b[0m         epoch,\n\u001b[0;32m     10\u001b[0m         gen_scheduler,\n\u001b[0;32m     11\u001b[0m         dis_scheduler,\n\u001b[0;32m     12\u001b[0m         latent_dim\u001b[39m=\u001b[39;49mlatent_dim,\n\u001b[0;32m     13\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[5], line 64\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(noise, generator, discriminator, optim_gen, optim_dis, epoch, gen_scheduler, dis_scheduler, latent_dim, device)\u001b[0m\n\u001b[0;32m     62\u001b[0m gener_noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mFloatTensor(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (img\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], latent_dim)))\n\u001b[0;32m     63\u001b[0m \u001b[39mprint\u001b[39m(noise\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 64\u001b[0m generated_imgs \u001b[39m=\u001b[39m generator(gener_noise)\n\u001b[0;32m     65\u001b[0m fake_valid \u001b[39m=\u001b[39m discriminator(generated_imgs)\n\u001b[0;32m     66\u001b[0m gener_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmean(fake_valid)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aashr\\Desktop\\research\\testing_grounds\\modelsxn.py:163\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, noise)\u001b[0m\n\u001b[0;32m    160\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcvup2(x)\n\u001b[0;32m    161\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape, \u001b[39m\"\u001b[39m\u001b[39mcvup 2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 163\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_block3(x)\n\u001b[0;32m    164\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape, \u001b[39m\"\u001b[39m\u001b[39mencoder block 3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape_for_out(\n\u001b[0;32m    167\u001b[0m     x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mview(\n\u001b[0;32m    168\u001b[0m         \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpsfac\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcvupmult\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, H, W\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    170\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aashr\\Desktop\\research\\testing_grounds\\modelutils.py:149\u001b[0m, in \u001b[0;36mcompleteEncoderModule.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    148\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding\n\u001b[1;32m--> 149\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtfenc(x)\n\u001b[0;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aashr\\Desktop\\research\\testing_grounds\\modelutils.py:100\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     99\u001b[0m     \u001b[39mfor\u001b[39;00m Encoder_Block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEncoder_Blocks:\n\u001b[1;32m--> 100\u001b[0m         x \u001b[39m=\u001b[39m Encoder_Block(x)\n\u001b[0;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aashr\\Desktop\\research\\testing_grounds\\modelutils.py:85\u001b[0m, in \u001b[0;36mEncoder_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     84\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(x)\n\u001b[1;32m---> 85\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x1)\n\u001b[0;32m     86\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(x)\n\u001b[0;32m     87\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(x2)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aashr\\Desktop\\research\\testing_grounds\\modelutils.py:56\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m dot \u001b[39m=\u001b[39m (q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m     55\u001b[0m attn \u001b[39m=\u001b[39m dot\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_dropout(attn)\n\u001b[0;32m     58\u001b[0m x \u001b[39m=\u001b[39m (attn \u001b[39m@\u001b[39m v)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(b, n, c)\n\u001b[0;32m     59\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 8.00 GiB total capacity; 4.69 GiB already allocated; 201.02 MiB free; 5.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 2000\n",
    "for epoch in range(epoch):\n",
    "    train(\n",
    "        noise,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        optim_gen,\n",
    "        optim_dis,\n",
    "        epoch,\n",
    "        gen_scheduler,\n",
    "        dis_scheduler,\n",
    "        latent_dim=latent_dim,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main_(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5378bde7bb2c4ef5b231e177b3c7c125": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ef80cd84e3f4f4fb320342c0ddbcf74": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f49c98ac29af4927a55e0998f7fd1b8c",
       "IPY_MODEL_c3b68c830c1743e4ae099886d4610e43"
      ],
      "layout": "IPY_MODEL_c738965f35bd43ed9ca1c627f3ec71e1"
     }
    },
    "757131b61efd4bacb3e3c6befcaaf286": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbff1579f370416d8c7116018ad4c0ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c3b68c830c1743e4ae099886d4610e43": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5378bde7bb2c4ef5b231e177b3c7c125",
      "placeholder": "​",
      "style": "IPY_MODEL_efea6db4dd9a4dd4b680d658051a7dee",
      "value": " 91.2M/91.2M [00:01&lt;00:00, 59.3MB/s]"
     }
    },
    "c738965f35bd43ed9ca1c627f3ec71e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efea6db4dd9a4dd4b680d658051a7dee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f49c98ac29af4927a55e0998f7fd1b8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_757131b61efd4bacb3e3c6befcaaf286",
      "max": 95628359,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bbff1579f370416d8c7116018ad4c0ae",
      "value": 95628359
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
