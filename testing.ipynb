{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from diff_aug import DiffAugment\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_feat, hid_feat=None, out_feat=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        if not hid_feat:\n",
    "            hid_feat = in_feat\n",
    "        if not out_feat:\n",
    "            out_feat = in_feat\n",
    "        self.fc1 = nn.Linear(in_feat, hid_feat)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hid_feat, out_feat)\n",
    "        self.droprateout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.droprateout(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, attention_dropout=0.0, proj_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dim = dim\n",
    "        # print(dim)\n",
    "        self.scale = 1.0 / dim**0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.out = nn.Sequential(nn.Linear(dim, dim), nn.Dropout(proj_dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.heads, c // self.heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        # print(\"qkv.shape:\", qkv.shape, \"Dim:\", self.dim, x.shape) \n",
    "        dot = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = dot.softmax(dim=-1)\n",
    "        attn = self.attention_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ImgPatches(nn.Module):\n",
    "    def __init__(self, input_channel=3, dim=768, patch_size=4):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            input_channel, dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patch_embed(img).flatten(2).transpose(1, 2)\n",
    "        return patches\n",
    "\n",
    "\n",
    "def UpSampling(x, H, W, psfac=2):\n",
    "    B, N, C = x.size()\n",
    "    assert N == H * W\n",
    "    x = x.permute(0, 2, 1)\n",
    "    x = x.view(-1, C, H, W)\n",
    "    x = nn.PixelShuffle(psfac)(x)\n",
    "    B, C, H, W = x.size()\n",
    "    x = x.view(-1, C, H * W)\n",
    "    x = x.permute(0, 2, 1)\n",
    "    return x, H, W\n",
    "\n",
    "\n",
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4, drop_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, drop_rate, drop_rate)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, dim * mlp_ratio, dropout=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x = x + self.attn(x1)\n",
    "        x2 = self.ln2(x)\n",
    "        x = x + self.mlp(x2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, depth, dim, heads, mlp_ratio=4, drop_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.Encoder_Blocks = nn.ModuleList(\n",
    "            [Encoder_Block(dim, heads, mlp_ratio, drop_rate) for i in range(depth)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for Encoder_Block in self.Encoder_Blocks:\n",
    "            x = Encoder_Block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_square(x: torch.Tensor):\n",
    "    B, N, C = x.size()\n",
    "    x = x.permute(0, 2, 1)\n",
    "    x = x.view(-1, C, int(N**.5), int(N**.5))\n",
    "    return x\n",
    "def to_flat(x: torch.Tensor):\n",
    "    B, C, H, W = x.size()\n",
    "    x = x.view(-1, C, H * W)\n",
    "    x = x.permute(0, 2, 1)\n",
    "    return x\n",
    "\n",
    "class convUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        print(in_channels, out_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x):\n",
    "        x = to_square(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.act(x)\n",
    "        x = to_flat(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 32\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"docstring for Generator\"\"\"\n",
    "\n",
    "    # ,device=device):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth1=5,\n",
    "        depth2=4,\n",
    "        depth3=2,\n",
    "        initial_size=8,\n",
    "        dim=384,\n",
    "        heads=4,\n",
    "        mlp_ratio=4,\n",
    "        drop_rate=0.0,\n",
    "        latent_dim=1024,\n",
    "        output_channels=1,\n",
    "        psfac=2,\n",
    "        cvupmult = 1\n",
    "    ):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # self.device = device\n",
    "        self.initial_size = initial_size\n",
    "        self.dim = dim\n",
    "        self.depth1 = depth1\n",
    "        self.depth2 = depth2\n",
    "        self.depth3 = depth3\n",
    "        self.heads = heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.droprate_rate = drop_rate\n",
    "        \n",
    "        self.mlp = nn.Linear(latent_dim, (self.initial_size**2) * self.dim)\n",
    "\n",
    "        self.positional_embedding_1 = nn.Parameter(\n",
    "            torch.zeros(1, (initial_size**2), self.dim)\n",
    "        )\n",
    "        self.positional_embedding_2 = nn.Parameter(\n",
    "            torch.zeros(1, (initial_size * (psfac**1)) ** 2, (self.dim // (psfac**2)))\n",
    "        )\n",
    "        self.positional_embedding_3 = nn.Parameter(\n",
    "            torch.zeros(1, (initial_size * (psfac**2)) ** 2, heads)\n",
    "        )\n",
    "\n",
    "        self.TransformerEncoder_encoder1 = TransformerEncoder(\n",
    "            depth=self.depth1,\n",
    "            dim=self.dim,\n",
    "            heads=self.heads,\n",
    "            mlp_ratio=self.mlp_ratio,\n",
    "            drop_rate=self.droprate_rate,\n",
    "        )\n",
    "        self.TransformerEncoder_encoder2 = TransformerEncoder(\n",
    "            depth=self.depth2,\n",
    "            dim=(self.dim // (psfac**2)),\n",
    "            heads=self.heads,\n",
    "            mlp_ratio=self.mlp_ratio,\n",
    "            drop_rate=self.droprate_rate,\n",
    "        )\n",
    "        self.TransformerEncoder_encoder3 = TransformerEncoder(\n",
    "            depth=self.depth3,\n",
    "            dim=heads,\n",
    "            heads=self.heads,\n",
    "            mlp_ratio=self.mlp_ratio,\n",
    "            drop_rate=self.droprate_rate,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Conv2d(self.dim // psfac**4, output_channels, 1, 1, 0)\n",
    "        )\n",
    "        self.psfac = psfac\n",
    "\n",
    "        self.cvup1 = convUp(in_channels = int(dim/(psfac**2)) ,out_channels= int(dim/(psfac**2))*cvupmult, kernel_size= 3,stride = 1, padding = 1)\n",
    "        self.cvdown = convUp(in_channels = int(dim/(psfac**4))*(cvupmult) ,out_channels= heads, kernel_size= 3,stride = 1, padding = 1)\n",
    "    def forward(self, noise):\n",
    "        H, W = self.initial_size, self.initial_size\n",
    "        x = self.mlp(noise).view(-1, self.initial_size**2, self.dim)\n",
    "        print(x.shape, \"initial MLP\")\n",
    "        x = x + self.positional_embedding_1\n",
    "        print(x.shape, \"pos embedding one\")\n",
    "        x = self.TransformerEncoder_encoder1(x)\n",
    "        print(x.shape, \"transformer encoder 1\")\n",
    "        x, H, W = UpSampling(x, H, W, psfac=self.psfac)\n",
    "        print(x.shape, \"upsampling 1\")\n",
    "        x = x + self.positional_embedding_2\n",
    "        print(x.shape, \"pos embedding 2\")\n",
    "        x = self.TransformerEncoder_encoder2(x)\n",
    "        print(x.shape, \"transformer encoder 2\")\n",
    "        x = self.cvup1(x)\n",
    "        print(x.shape)\n",
    "        x, H, W = UpSampling(x, H, W, psfac=self.psfac)\n",
    "        print(x.shape, \"upsampling 2\")\n",
    "        x = x + self.positional_embedding_3\n",
    "        print(x.shape, \"pos embedding 3\")\n",
    "        x = self.TransformerEncoder_encoder3(x)\n",
    "        print(x.shape, \"transformer encoder 3\")\n",
    "        x = self.linear(x.permute(0, 2, 1).view(-1, self.dim // (self.psfac**4), H, W))\n",
    "        print(x.shape, \"reshaping for output\")\n",
    "        return x\n",
    "    \n",
    "    def generate(self, num_images):\n",
    "        return self.forward(torch.cuda.FloatTensor(np.random.normal(0, 1, (num_images, 256))))\n",
    "generator = Generator(\n",
    "        depth1=1,\n",
    "        depth2=1,\n",
    "        depth3=1,\n",
    "        initial_size=8,\n",
    "        dim=128*2,\n",
    "        heads=2,\n",
    "        mlp_ratio=1,\n",
    "        drop_rate=0.5,\n",
    "        latent_dim=256,\n",
    "        psfac=4,\n",
    "        cvupmult = 2\n",
    "    ).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 256]) initial MLP\n",
      "torch.Size([2, 64, 256]) pos embedding one\n",
      "torch.Size([2, 64, 256]) transformer encoder 1\n",
      "torch.Size([2, 1024, 16]) upsampling 1\n",
      "torch.Size([2, 1024, 16]) pos embedding 2\n",
      "torch.Size([2, 1024, 16]) transformer encoder 2\n",
      "torch.Size([2, 1024, 32])\n",
      "torch.Size([2, 16384, 2]) upsampling 2\n",
      "torch.Size([2, 16384, 2]) pos embedding 3\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 8.00 GiB total capacity; 4.06 GiB already allocated; 2.86 GiB free; 4.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generator\u001b[39m.\u001b[39;49mgenerate(\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[4], line 100\u001b[0m, in \u001b[0;36mGenerator.generate\u001b[1;34m(self, num_images)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, num_images):\n\u001b[1;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mFloatTensor(np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mnormal(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, (num_images, \u001b[39m256\u001b[39;49m))))\n",
      "Cell \u001b[1;32mIn[4], line 93\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, noise)\u001b[0m\n\u001b[0;32m     91\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding_3\n\u001b[0;32m     92\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape, \u001b[39m\"\u001b[39m\u001b[39mpos embedding 3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTransformerEncoder_encoder3(x)\n\u001b[0;32m     94\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape, \u001b[39m\"\u001b[39m\u001b[39mtransformer encoder 3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpsfac\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m), H, W))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[2], line 95\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m Encoder_Block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEncoder_Blocks:\n\u001b[1;32m---> 95\u001b[0m         x \u001b[39m=\u001b[39m Encoder_Block(x)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[2], line 80\u001b[0m, in \u001b[0;36mEncoder_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     79\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(x)\n\u001b[1;32m---> 80\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x1)\n\u001b[0;32m     81\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(x)\n\u001b[0;32m     82\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(x2)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[2], line 37\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m q, k, v \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39m# print(\"qkv.shape:\", qkv.shape, \"Dim:\", self.dim, x.shape) \u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m dot \u001b[39m=\u001b[39m (q \u001b[39m@\u001b[39;49m k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)) \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale\n\u001b[0;32m     38\u001b[0m attn \u001b[39m=\u001b[39m dot\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_dropout(attn)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 8.00 GiB total capacity; 4.06 GiB already allocated; 2.86 GiB free; 4.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "generator.generate(2).shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
