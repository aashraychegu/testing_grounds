{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinexpDL import *\n",
    "runnum = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where the actual ML algo starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LeakyReLU\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Sigmoid\n",
    "from torch import flatten\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorOld(nn.Module):\n",
    "    def __init__(self, latent_dim=100, batchnorm=True):\n",
    "        super(GeneratorOld, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batchnorm = batchnorm\n",
    "        # first set of CONVT => RELU => BN\n",
    "        self.linear1 = nn.Linear(self.latent_dim, 256*7*7, bias=False)\n",
    "        self.bn1d1 = nn.BatchNorm1d(256*7*7) if self.batchnorm else None\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv1 = nn.ConvTranspose2d(\n",
    "                in_channels=256,\n",
    "                out_channels=128,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "                bias=False)\n",
    "        self.bn2d1 = nn.BatchNorm2d(128) if self.batchnorm else None\n",
    "\n",
    "        self.conv2 = nn.ConvTranspose2d(\n",
    "                in_channels=128,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False)\n",
    "        self.bn2d2 = nn.BatchNorm2d(64) if self.batchnorm else None\n",
    "\n",
    "        self.conv2dot5 = nn.ConvTranspose2d(\n",
    "                in_channels=64,\n",
    "                out_channels=32,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False)\n",
    "        self.bn2d2dot5 = nn.BatchNorm2d(32) if self.batchnorm else None\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(\n",
    "                in_channels=32,\n",
    "                out_channels=16,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=1,\n",
    "            kernel_size=2,\n",
    "            padding=1\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "        intermediate = self.linear1(input_tensor)\n",
    "        intermediate = self.bn1d1(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = intermediate.view((-1, 256, 7, 7))\n",
    "\n",
    "        intermediate = self.conv1(intermediate)\n",
    "        if self.batchnorm:\n",
    "            intermediate = self.bn2d1(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "\n",
    "        intermediate = self.conv2(intermediate)\n",
    "        if self.batchnorm:\n",
    "            intermediate = self.bn2d2(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "\n",
    "        intermediate = self.conv2dot5(intermediate)\n",
    "        if self.batchnorm:\n",
    "            intermediate = self.bn2d2dot5(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "\n",
    "        intermediate = self.conv3(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.conv4(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.conv5(intermediate)\n",
    "        output_tensor = self.sigmoid(intermediate)\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"A discriminator for discerning real from generated images.\n",
    "        Images must be single-channel and 28x28 pixels.\n",
    "        Output activation is Sigmoid.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self._init_modules()\n",
    "\n",
    "    def _init_modules(self):\n",
    "        \"\"\"Initialize the modules.\"\"\"\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=64,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=2,\n",
    "                bias=True)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.dropout_2d = nn.Dropout2d(0.4)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=2,\n",
    "                bias=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(128*16*16, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
    "        # print(input_tensor.shape)\n",
    "        intermediate = self.conv1(input_tensor)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.dropout_2d(intermediate)\n",
    "        # print(intermediate.shape)\n",
    "        intermediate = self.conv2(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.dropout_2d(intermediate)\n",
    "        # print(intermediate.shape)\n",
    "        intermediate = intermediate.view((-1, 128*16*16))\n",
    "        # print(intermediate.shape)\n",
    "        intermediate = self.linear1(intermediate)\n",
    "        output_tensor = self.sigmoid(intermediate)\n",
    "        # print(output_tensor.shape,\"\\n\")\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu,latent_dim = 1,ngf = 64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # turns the 'interesting and 'informative' ' 2 parameters into slightly more informative 128 parameters\n",
    "        self.lin1 = nn.Linear(latent_dim, 128)\n",
    "        self.lin2 = nn.Linear(128, (ngf*16)*16*16)\n",
    "        # input is Z, going into a convolution\n",
    "        self.btn0 = nn.BatchNorm2d(ngf*16)\n",
    "        self.ctp1 = nn.ConvTranspose2d(\n",
    "            in_channels=ngf*16,\n",
    "            out_channels=ngf * 8,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.btn1 = nn.BatchNorm2d(ngf * 8)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        self.ctp2 = nn.ConvTranspose2d(\n",
    "            in_channels=ngf * 8,\n",
    "            out_channels=ngf * 4,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.btn2 = nn.BatchNorm2d(ngf * 4)\n",
    "        # nn.ReLU(True),\n",
    "        # state size. (ngf*4) x 8 x 8\n",
    "        self.ctp3 = nn.ConvTranspose2d(\n",
    "            in_channels=ngf * 4,\n",
    "            out_channels=ngf * 2,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.btn3 = nn.BatchNorm2d(ngf * 2)\n",
    "        # state size. (ngf*2) x 16 x 16\n",
    "        self.ctp4 = nn.ConvTranspose2d(\n",
    "            in_channels=ngf * 2,\n",
    "            out_channels=ngf,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.btn4 = nn.BatchNorm2d(ngf)\n",
    "        # state size. (ngf) x 32 x 32\n",
    "        self.ctp5 = nn.ConvTranspose2d(\n",
    "            in_channels=ngf,\n",
    "            out_channels=1,\n",
    "            kernel_size=4,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "        # state size. (nc) x 64 x 64\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.to(self.device).float() \n",
    "        # print(f\"IN {inp.shape = }\")\n",
    "        inm = self.lin1(inp)\n",
    "        # print(f\"T1 {inm.shape = }\")\n",
    "        inm = self.relu(inm)\n",
    "        inm = self.lin2(inm)\n",
    "        # print(f\"T2 {inm.shape = }\")\n",
    "        inm = self.relu(inm)\n",
    "        inm = inm.view((-1, 64*16, 16, 16))\n",
    "        # print(f\"RV {inm.shape = }\")\n",
    "\n",
    "        inm = self.ctp1(inm)\n",
    "        inm = self.btn1(inm)\n",
    "        inm = self.relu(inm)\n",
    "        # print(f\"C1 {inm.shape = }\")\n",
    "\n",
    "        inm = self.ctp2(inm)\n",
    "        inm = self.btn2(inm)\n",
    "        inm = self.relu(inm)\n",
    "        # print(f\"C2 {inm.shape = }\")\n",
    "\n",
    "        inm = self.ctp3(inm)\n",
    "        inm = self.btn3(inm)\n",
    "        inm = self.relu(inm)\n",
    "        # print(f\"C3 {inm.shape = }\")\n",
    "\n",
    "        inm = self.ctp4(inm)\n",
    "        inm = self.btn4(inm)\n",
    "        inm = self.relu(inm)\n",
    "        # print(f\"C4 {inm.shape = }\")\n",
    "\n",
    "        inm = self.ctp5(inm)\n",
    "        fin = self.relu(inm)\n",
    "        # print(f\"OT {fin.shape = }\",\"\\n\")\n",
    "        return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, latent_dim, noise_fn, dataloader,\n",
    "                 batch_size=32, device='cpu', lr_d=1e-3, lr_g=2e-4):\n",
    "        \"\"\"A very basic DCGAN class for generating MNIST digits\n",
    "        Args:\n",
    "            generator: a Ganerator network\n",
    "            discriminator: A Discriminator network\n",
    "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
    "            dataloader: a pytorch dataloader for loading images\n",
    "            batch_size: training batch size. Must match that of dataloader\n",
    "            device: cpu or CUDA\n",
    "            lr_d: learning rate for the discriminator\n",
    "            lr_g: learning rate for the generator\n",
    "        \"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.generator = Generator(latent_dim).to(device)\n",
    "        self.discriminator = Discriminator().to(device)\n",
    "        self.noise_fn = noise_fn\n",
    "        self.dataloader = dataloader\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optim_d = optim.Adam(self.discriminator.parameters(),\n",
    "                                  lr=lr_d, betas=(0.5, 0.999))\n",
    "        self.optim_g = optim.Adam(self.generator.parameters(),\n",
    "                                  lr=lr_g, betas=(0.5, 0.999))\n",
    "        self.target_ones = torch.ones((batch_size, 1), device=device)\n",
    "        self.target_zeros = torch.zeros((batch_size, 1), device=device)\n",
    "\n",
    "    def generate_samples(self, latent_vec=None, num=None):\n",
    "        \"\"\"Sample images from the generator.\n",
    "        Images are returned as a 4D tensor of values between -1 and 1.\n",
    "        Dimensions are (number, channels, height, width). Returns the tensor\n",
    "        on cpu.\n",
    "        Args:\n",
    "            latent_vec: A pytorch latent vector or None\n",
    "            num: The number of samples to generate if latent_vec is None\n",
    "        If latent_vec and num are None then use self.batch_size\n",
    "        random latent vectors.\n",
    "        \"\"\"\n",
    "        num = self.batch_size if num is None else num\n",
    "        latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
    "        with torch.no_grad():\n",
    "            samples = self.generator(latent_vec)\n",
    "        samples = samples.cpu()  # move images to cpu\n",
    "        return samples,latent_vec\n",
    "\n",
    "    def train_step_generator(self):\n",
    "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
    "        self.generator.zero_grad()\n",
    "\n",
    "        latent_vec = self.noise_fn(self.batch_size)\n",
    "        generated = self.generator(latent_vec)\n",
    "        classifications = self.discriminator(generated)\n",
    "        loss = self.criterion(classifications, self.target_ones)\n",
    "        loss.backward()\n",
    "        self.optim_g.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train_step_discriminator(self, real_samples):\n",
    "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
    "        local_batch_size = real_samples.shape[0]\n",
    "        target_ones = torch.ones((local_batch_size, 1), device=self.device)\n",
    "        target_zeros = torch.zeros(\n",
    "            (local_batch_size, 1), device=self.device)\n",
    "        \n",
    "        self.discriminator.zero_grad()\n",
    "        # real samples\n",
    "        pred_real = self.discriminator(real_samples)\n",
    "        # print(real_samples.shape, pred_real.shape,target_ones.shape)\n",
    "        loss_real = self.criterion(pred_real, target_ones)\n",
    "        \n",
    "        # generated samples\n",
    "        latent_vec = self.noise_fn(local_batch_size)\n",
    "        with torch.no_grad():\n",
    "            fake_samples = self.generator(latent_vec)\n",
    "        # print(f\"{fake_samples.shape = }\")\n",
    "        pred_fake = self.discriminator(fake_samples)\n",
    "        loss_fake = self.criterion(pred_fake, target_zeros)\n",
    "        \n",
    "        # combine\n",
    "        loss = (loss_real + loss_fake) / 2\n",
    "        loss.backward()\n",
    "        self.optim_d.step()\n",
    "        return loss_real.item(), loss_fake.item()\n",
    "\n",
    "    def train_epoch(self, print_frequency=10, max_steps=0):\n",
    "        \"\"\"Train both networks for one epoch and return the losses.\n",
    "        Args:\n",
    "            print_frequency (int): print stats every `print_frequency` steps.\n",
    "            max_steps (int): End epoch after `max_steps` steps, or set to 0\n",
    "                             to do the full epoch.\n",
    "        \"\"\"\n",
    "        loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
    "        for batch, real_samples in enumerate(self.dataloader):\n",
    "            real_samples = real_samples.to(self.device)\n",
    "\n",
    "            #! This is the important step\n",
    "            ldr_, ldf_ = self.train_step_discriminator(real_samples)\n",
    "            loss_g_running += self.train_step_generator()\n",
    "            \n",
    "            loss_d_real_running += ldr_\n",
    "            loss_d_fake_running += ldf_\n",
    "            print(f\"{batch+1}/{len(self.dataloader)}:\"\n",
    "                  f\" G={loss_g_running / (batch+1):.3f},\"\n",
    "                  f\" Dr={loss_d_real_running / (batch+1):.3f},\"\n",
    "                  f\" Df={loss_d_fake_running / (batch+1):.3f}\",\n",
    "                  end='\\r',\n",
    "                  flush=True)\n",
    "        if print_frequency:\n",
    "            print()\n",
    "            \n",
    "        loss_g_running /= batch\n",
    "        loss_d_real_running /= batch\n",
    "        loss_d_fake_running /= batch\n",
    "        return (loss_g_running, (loss_d_real_running, loss_d_fake_running))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "cuda\n",
      "self.pspace.shape = (1000,) | self.__len__() = 1000\n"
     ]
    }
   ],
   "source": [
    "runnum += 1\n",
    "print(runnum)\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "batch_size = 16\n",
    "latent_dim = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "pstart = 1\n",
    "pend = 2\n",
    "ds = specGds(\"cuda:0\", pstart, pend)\n",
    "dataloader = DataLoader(ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "                                            \n",
    "\n",
    "def noise_fn(x):\n",
    "    return torch.abs(torch.normal(mean = (pstart+pend)/2, std = (pend - pstart)/4, size = (x,latent_dim)))\n",
    "\n",
    "\n",
    "gan = DCGAN(latent_dim, noise_fn, dataloader, device=device, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/63: G=0.701, Dr=0.699, Df=0.700\n",
      "Epoch 1; Elapsed time = 5s / 0.08333333333333333 minutes; Time for this epoch: 5.811548948287964s / 0.09685914913813273 minutes;\n",
      "tensor([2.0286])\n",
      "tensor([1.6278])\n",
      "tensor([1.5836])\n",
      "tensor([1.2887])\n",
      "tensor([1.6266])\n",
      "tensor([1.1876])\n",
      "tensor([1.5845])\n",
      "tensor([1.3860])\n",
      "tensor([1.3191])\n",
      "tensor([1.6802])\n",
      "tensor([1.7476])\n",
      "tensor([1.5997])\n",
      "tensor([1.8178])\n",
      "tensor([1.2925])\n",
      "tensor([1.4224])\n",
      "tensor([1.6571])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time()\n",
    "epochs = 1\n",
    "prevlapsed = start\n",
    "for i in range(epochs):\n",
    "    gan.train_epoch()\n",
    "    ctime = time()\n",
    "    print(f\"Epoch {i+1}; Elapsed time = {int(ctime - start)}s / {int(ctime - start)/60} minutes; Time for this epoch: {ctime-prevlapsed}s / {(ctime-prevlapsed)/60} minutes;\")\n",
    "    prevlapsed = ctime\n",
    "\n",
    "images,vec = gan.generate_samples()\n",
    "for i,v in zip(images,vec):\n",
    "    print(v)\n",
    "    plt.pcolormesh(i[0].cpu().detach().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
