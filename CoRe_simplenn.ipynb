{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoReDataLoader import  dataset,dataloader, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = metrics.Accuracy(task = \"multiclass\",num_classes = len(dataset.eoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self,input_length = maxlen):\n",
    "        super().__init__()\n",
    "        self.inlayer = nn.Linear(input_length,4096)\n",
    "        self.bnorm = nn.BatchNorm1d(4096)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.inter1 = nn.Linear(4096,2056)\n",
    "        self.inter2 = nn.Linear(2056,1024)\n",
    "        self.inter3 = nn.Linear(1024,1024)\n",
    "        self.output = nn.Linear(1024,len(dataset.eoss))\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        inp = inp.to(torch.float32)\n",
    "        itn = self.inlayer(inp)\n",
    "        itn = self.bnorm(itn)\n",
    "        itn = self.silu(itn)\n",
    "        itn = self.silu(self.inter1(itn))\n",
    "        itn = self.silu(self.inter2(itn))\n",
    "        itn = self.silu(self.inter3(itn))\n",
    "        itn = self.silu(self.output(itn))\n",
    "        return itn\n",
    "net = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (inlayer): Linear(in_features=40817, out_features=4096, bias=True)\n",
       "  (bnorm): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (silu): SiLU()\n",
       "  (inter1): Linear(in_features=4096, out_features=2056, bias=True)\n",
       "  (inter2): Linear(in_features=2056, out_features=1024, bias=True)\n",
       "  (inter3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (output): Linear(in_features=1024, out_features=19, bias=True)\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr = 1,betas = (0.9,0.999))\n",
    "epochs = 100\n",
    "net.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100 1/148 loss = 4376348160.0 accuracy = 0.0\n",
      "1/100 2/148 loss = 49495433216.0 accuracy = 0.0625\n",
      "1/100 3/148 loss = 21648537600.0 accuracy = 0.125\n",
      "1/100 4/148 loss = 13987555328.0 accuracy = 0.5\n",
      "1/100 5/148 loss = 8971236352.0 accuracy = 0.1875\n",
      "1/100 6/148 loss = 4399267328.0 accuracy = 0.75\n",
      "1/100 7/148 loss = 1845633280.0 accuracy = 0.0\n",
      "1/100 8/148 loss = 1872699264.0 accuracy = 0.3125\n",
      "1/100 9/148 loss = 750020032.0 accuracy = 0.0625\n",
      "1/100 10/148 loss = 1129878784.0 accuracy = 0.0625\n",
      "1/100 11/148 loss = 49428832256.0 accuracy = 0.1875\n",
      "1/100 12/148 loss = 2726099712.0 accuracy = 0.25\n",
      "1/100 13/148 loss = 259686464.0 accuracy = 0.125\n",
      "1/100 14/148 loss = 534744416.0 accuracy = 0.1875\n",
      "1/100 15/148 loss = 215072208.0 accuracy = 0.125\n",
      "1/100 16/148 loss = 334612512.0 accuracy = 0.1875\n",
      "1/100 17/148 loss = 491064960.0 accuracy = 0.375\n",
      "1/100 18/148 loss = 2960518656.0 accuracy = 0.125\n",
      "1/100 19/148 loss = 373913984.0 accuracy = 0.4375\n",
      "1/100 20/148 loss = 406469728.0 accuracy = 0.375\n",
      "1/100 21/148 loss = 311268224.0 accuracy = 0.375\n",
      "1/100 22/148 loss = 515779392.0 accuracy = 0.0\n",
      "1/100 23/148 loss = 616685120.0 accuracy = 0.0\n",
      "1/100 24/148 loss = 239096704.0 accuracy = 0.1875\n",
      "1/100 25/148 loss = 181705760.0 accuracy = 0.0\n",
      "1/100 26/148 loss = 62281336.0 accuracy = 0.0\n",
      "1/100 27/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 28/148 loss = 38538336.0 accuracy = 0.1875\n",
      "1/100 29/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 30/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 31/148 loss = 2.944438934326172 accuracy = 0.125\n",
      "1/100 32/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 33/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 34/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 35/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 36/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 37/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 38/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 39/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 40/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 41/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 42/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 43/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 44/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 45/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 46/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 47/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 48/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 49/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 50/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 51/148 loss = 2.944438934326172 accuracy = 0.125\n",
      "1/100 52/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 53/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 54/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 55/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 56/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 57/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 58/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 59/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 60/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 61/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 62/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 63/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 64/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 65/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 66/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 67/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 68/148 loss = 2.944438934326172 accuracy = 0.125\n",
      "1/100 69/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 70/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 71/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 72/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 73/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 74/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 75/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 76/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 77/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 78/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 79/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 80/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 81/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 82/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 83/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 84/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 85/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 86/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 87/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 88/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 89/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 90/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 91/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 92/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 93/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 94/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 95/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 96/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 97/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 98/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 99/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 100/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 101/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 102/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 103/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 104/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 105/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 106/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 107/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 108/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 109/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 110/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 111/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 112/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 113/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 114/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 115/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 116/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 117/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 118/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 119/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 120/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 121/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 122/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 123/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 124/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 125/148 loss = 2.944438934326172 accuracy = 0.0625\n",
      "1/100 126/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 127/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "1/100 128/148 loss = 2.944438934326172 accuracy = 0.0\n",
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs,params[:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mlong))\n\u001b[0;32m      9\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 10\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     11\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(dataloader)\u001b[39m}\u001b[39;00m\u001b[39m loss = \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m accuracy = \u001b[39m\u001b[39m{\u001b[39;00mmetric(outputs,params[:,\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,end \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m,flush \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\aashr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\aashr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\aashr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\aashr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\aashr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:412\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m--> 412\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "correct = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch, (ts,params) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(ts)\n",
    "        # print(outputs.shape,params[:,0].to(torch.long).shape)\n",
    "        loss = criterion(outputs,params[:,0].to(torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        print(f\"{epoch+1}/{epochs} {batch+1}/{len(dataloader)} loss = {loss.item()} accuracy = {metric(outputs,params[:,0])*100}%\\n\",end = \"\\r\",flush = True)\n",
    "        correct = 0\n",
    "    print(f\"Epoch finished: {epoch+1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b5579af3731f6410547d30894d3cfa69af3dd817cb29011736dbe918371f0ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
