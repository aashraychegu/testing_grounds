{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoRe_Dataloader_ECSG import load_pth_file,load_raw_from_pth_file\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import math\n",
    "import torchinfo\n",
    "import time\n",
    "import numpy as np\n",
    "import wandb\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sgram_ds, raw_param_ds = load_raw_from_pth_file()\n",
    "def get_new_test_train(p = 0.3):\n",
    "    assert p < 1\n",
    "    xtrain,xtest,ytrain,ytest = train_test_split(raw_sgram_ds.cpu().numpy(),raw_param_ds.cpu().numpy(),test_size = p)\n",
    "    train_dataset = TensorDataset(torch.tensor(xtrain),torch.tensor(ytrain))\n",
    "    if p == 0:\n",
    "        test_dataset = TensorDataset(torch.tensor(xtrain),torch.tensor(ytrain))\n",
    "    else:\n",
    "        test_dataset = TensorDataset(torch.tensor(xtest), torch.tensor(ytest))\n",
    "    return DataLoader(train_dataset,batch_size = 8, shuffle = True), DataLoader(test_dataset,batch_size = 64,shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as metrics\n",
    "acc = metrics.Accuracy(task=\"multiclass\",num_classes=19).to(\"cuda:0\")\n",
    "auroc = metrics.AUROC(task = \"multiclass\",num_classes=19).to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_accuracy(model:torch.nn.Module,dl:DataLoader):\n",
    "    model.eval()\n",
    "    raw_output = []\n",
    "    parameters = []\n",
    "    with torch.no_grad():\n",
    "        for batch,(sg,params) in enumerate(dl):\n",
    "            sg = sg.to(\"cuda:0\").to(torch.float)\n",
    "            sgsh = sg.shape\n",
    "            sg = sg.view(sgsh[0], 1, sgsh[1], sgsh[2])\n",
    "\n",
    "            params = params[:,0].to(\"cuda:0\").to(torch.long)\n",
    "            \n",
    "            raw_output.append(model(sg).detach())\n",
    "            parameters.append(params)\n",
    "            \n",
    "    model.train()\n",
    "    output = torch.vstack(raw_output)\n",
    "    parameters = torch.hstack(parameters)\n",
    "    accuracy = acc(output,parameters)\n",
    "    auc = auroc(output,parameters)\n",
    "    return accuracy,auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vit\n",
    "# import vit_pytorch\n",
    "from vit_pytorch import vit_for_small_dataset as vit_sd\n",
    "from vit_pytorch import vit as simple_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    # return simple_vit.ViT(image_size=400,\n",
    "    #                patch_size=20,\n",
    "    #                num_classes=19,\n",
    "    #                dim=int(1024/2),\n",
    "    #                depth=2,\n",
    "    #                heads=8,\n",
    "    #                mlp_dim=int(2048/2),\n",
    "    #                channels=1).to(\"cuda:0\")\n",
    "    return vit_sd.ViT(image_size=400,\n",
    "                   patch_size=20,\n",
    "                   num_classes=19,\n",
    "                   dim=1024,\n",
    "                   depth=3,\n",
    "                   heads=16,\n",
    "                   mlp_dim=int(2048/2),\n",
    "                   dropout = 0.1,\n",
    "                   emb_dropout = 0.1,\n",
    "                   channels=1).to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1761, -0.1176, -0.0582, -0.0227, -0.6981, -0.0344,  0.2345, -0.5933,\n",
      "          1.1695, -0.4646,  0.0839,  0.9832, -0.1001, -0.7190,  0.0979,  0.1236,\n",
      "         -0.8274, -0.3403, -0.2315]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashr\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ViT                                                [1, 19]                   411,648\n",
       "├─SPT: 1-1                                         [1, 40, 1024]             --\n",
       "│    └─Sequential: 2-1                             [1, 40, 1024]             --\n",
       "│    │    └─Rearrange: 3-1                         [1, 40, 2000]             --\n",
       "│    │    └─LayerNorm: 3-2                         [1, 40, 2000]             4,000\n",
       "│    │    └─Linear: 3-3                            [1, 40, 1024]             2,049,024\n",
       "├─Dropout: 1-2                                     [1, 41, 1024]             --\n",
       "├─Transformer: 1-3                                 [1, 41, 1024]             --\n",
       "│    └─ModuleList: 2-2                             --                        --\n",
       "│    │    └─ModuleList: 3-4                        --                        6,298,625\n",
       "│    │    └─ModuleList: 3-5                        --                        6,298,625\n",
       "│    │    └─ModuleList: 3-6                        --                        6,298,625\n",
       "├─Identity: 1-4                                    [1, 1024]                 --\n",
       "├─Sequential: 1-5                                  [1, 19]                   --\n",
       "│    └─LayerNorm: 2-3                              [1, 1024]                 2,048\n",
       "│    └─Linear: 2-4                                 [1, 19]                   19,475\n",
       "====================================================================================================\n",
       "Total params: 21,382,070\n",
       "Trainable params: 21,382,070\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 20.97\n",
       "====================================================================================================\n",
       "Input size (MB): 0.06\n",
       "Forward/backward pass size (MB): 9.04\n",
       "Params size (MB): 83.88\n",
       "Estimated Total Size (MB): 92.98\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = init_model()\n",
    "\n",
    "\n",
    "img = torch.randn(1,1, 400,400).to(\"cuda:0\")\n",
    "\n",
    "preds = model(img)  # (1, 1000)\n",
    "print(preds)\n",
    "torchinfo.summary(model, input_size=(1,1, 400, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "startlr = 3e-5\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=startlr)\n",
    "optimizer1 = optim.NAdam(params=model.parameters(), lr=startlr)\n",
    "step_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=[250], gamma=0.9)\n",
    "# at the end of 600 epochs, the learning rate is 0.000,002,62\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=1, gamma=0.988)\n",
    "scheduler_pl = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', factor=0.7, patience=35, verbose=True)\n",
    "lossfn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(config,train_dl,test_dl,adam = True,nadam = False):\n",
    "    tot_acc,auc = new_accuracy(model=model,dl = test_dl)\n",
    "    max_acc = -1\n",
    "    max_auc = -1\n",
    "    for epoch in range(1,config.epochs+1):\n",
    "        btime = time.time()\n",
    "        ldl = len(train_dl)\n",
    "        for batch,(sg,params) in enumerate(train_dl):\n",
    "            stime = time.time()\n",
    "            sg = sg.to(\"cuda:0\").to(torch.float)\n",
    "            sgsh = sg.shape\n",
    "            sg = sg.view(sgsh[0],1,sgsh[1],sgsh[2])\n",
    "            params = params[:,0].to(\"cuda:0\").to(torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sg)\n",
    "            loss = lossfn(outputs,params)\n",
    "            loss.backward()\n",
    "            optimizer.step() if adam else None\n",
    "            optimizer1.step() if nadam else None\n",
    "            wandb.log({\"loss\":loss.item(),\"batch_accuracy\":acc(outputs,params),\"lr\":scheduler.get_last_lr()[0],\"epoch\":epoch})\n",
    "            print(f\"{epoch:5}/{config.epochs:5} // {batch:5}/{ldl:5} | Loss: {loss.item():2.4},batch_accuracy:{acc(outputs,params):3.4}, last_total_accuracy: {tot_acc}, Maximum Accuracy {max_acc} last AUROC {auc} Max AUC {max_auc} lr:{scheduler.get_last_lr()[0]:1.5},Time per Batch: {time.time()-stime:1.2} seconds     \",end = \"\\r\",flush=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        tot_acc, auc = new_accuracy(model=model, dl=test_dl)\n",
    "        scheduler.step()\n",
    "        step_scheduler.step()\n",
    "        scheduler_pl.step(tot_acc)\n",
    "        if(tot_acc > max_acc):\n",
    "            max_acc = tot_acc\n",
    "            config.best_model = model.state_dict()\n",
    "            try:\n",
    "                torch.save(config.best_model, f\"./saved_models/ViT/best_model_state_dict_ViT_for{config.run_name}_stime_{config.start_time.replace(':', '-')}__acc_{max_acc}__auc_{auc}.pt\")\n",
    "            except:  pass    \n",
    "            print(\"\\nSAVING MODEL\")\n",
    "        max_auc = max(max_auc,auc)\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.epochs} finished. Total accuracy: {tot_acc:3.5} AUROC: {auc} Time per Epoch: {time.time()-btime:1.5}\")\n",
    "\n",
    "        wandb.log({\"epoch\":epoch,\"accuracy\":tot_acc,\"max_accuracy\":max_acc,\"lr\":scheduler.get_last_lr()[0],\"auroc\":auc})\n",
    "    return max_acc,max_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maashraychegu\u001b[0m (\u001b[33malabs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\aashr\\Desktop\\research\\testing_grounds\\wandb\\run-20230308_205618-be6a28s8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alabs/simple_vision_transformer_forsmalldatasets_validation_test/runs/be6a28s8\" target=\"_blank\">vivid-glitter-4</a></strong> to <a href=\"https://wandb.ai/alabs/simple_vision_transformer_forsmalldatasets_validation_test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/alabs/simple_vision_transformer_forsmalldatasets_validation_test\" target=\"_blank\">https://wandb.ai/alabs/simple_vision_transformer_forsmalldatasets_validation_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/alabs/simple_vision_transformer_forsmalldatasets_validation_test/runs/be6a28s8\" target=\"_blank\">https://wandb.ai/alabs/simple_vision_transformer_forsmalldatasets_validation_test/runs/be6a28s8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashr\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/  500 //   280/  281 | Loss: 1.497,batch_accuracy:0.8333, last_total_accuracy: 0.008403361774981022, Maximum Accuracy -1 last AUROC 0.35685214400291443 Max AUC -1 lr:3e-05,Time per Batch: 0.11 seconds     \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 2/500 finished. Total accuracy: 0.43697 AUROC: 0.44416216015815735 Time per Epoch: 42.132\n",
      "    2/  500 //   280/  281 | Loss: 1.103,batch_accuracy:0.5, last_total_accuracy: 0.43697479367256165, Maximum Accuracy 0.43697479367256165 last AUROC 0.44416216015815735 Max AUC 0.44416216015815735 lr:2.964e-05,Time per Batch: 0.11 seconds        \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 3/500 finished. Total accuracy: 0.47899 AUROC: 0.45062416791915894 Time per Epoch: 41.462\n",
      "    3/  500 //   280/  281 | Loss: 2.442,batch_accuracy:0.3333, last_total_accuracy: 0.4789915978908539, Maximum Accuracy 0.4789915978908539 last AUROC 0.45062416791915894 Max AUC 0.45062416791915894 lr:2.9284e-05,Time per Batch: 0.11 seconds     \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 4/500 finished. Total accuracy: 0.61345 AUROC: 0.5730961561203003 Time per Epoch: 41.405\n",
      "    4/  500 //   280/  281 | Loss: 0.9475,batch_accuracy:0.5, last_total_accuracy: 0.6134454011917114, Maximum Accuracy 0.6134454011917114 last AUROC 0.5730961561203003 Max AUC 0.5730961561203003 lr:2.8933e-05,Time per Batch: 0.11 seconds       \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 5/500 finished. Total accuracy: 0.68908 AUROC: 0.608849048614502 Time per Epoch: 41.875\n",
      "    5/  500 //   280/  281 | Loss: 0.9715,batch_accuracy:0.6667, last_total_accuracy: 0.6890756487846375, Maximum Accuracy 0.6890756487846375 last AUROC 0.608849048614502 Max AUC 0.608849048614502 lr:2.8586e-05,Time per Batch: 0.11 seconds     \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 6/500 finished. Total accuracy: 0.69748 AUROC: 0.6091717481613159 Time per Epoch: 44.833\n",
      "    6/  500 //   280/  281 | Loss: 1.126,batch_accuracy:0.6667, last_total_accuracy: 0.6974790096282959, Maximum Accuracy 0.6974790096282959 last AUROC 0.6091717481613159 Max AUC 0.6091717481613159 lr:2.8243e-05,Time per Batch: 0.1 seconds      \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 7/500 finished. Total accuracy: 0.76471 AUROC: 0.6330211162567139 Time per Epoch: 39.57\n",
      "    7/  500 //   280/  281 | Loss: 0.04263,batch_accuracy:1.0, last_total_accuracy: 0.7647058963775635, Maximum Accuracy 0.7647058963775635 last AUROC 0.6330211162567139 Max AUC 0.6330211162567139 lr:2.7904e-05,Time per Batch: 0.12 seconds      \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 8/500 finished. Total accuracy: 0.77311 AUROC: 0.6423913240432739 Time per Epoch: 39.864\n",
      "    8/  500 //   280/  281 | Loss: 1.174,batch_accuracy:0.6667, last_total_accuracy: 0.7731092572212219, Maximum Accuracy 0.7731092572212219 last AUROC 0.6423913240432739 Max AUC 0.6423913240432739 lr:2.7569e-05,Time per Batch: 0.11 seconds     \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 9/500 finished. Total accuracy: 0.84034 AUROC: 0.6558209657669067 Time per Epoch: 43.994\n",
      "    9/  500 //   280/  281 | Loss: 0.8924,batch_accuracy:0.6667, last_total_accuracy: 0.8403361439704895, Maximum Accuracy 0.8403361439704895 last AUROC 0.6558209657669067 Max AUC 0.6558209657669067 lr:2.7238e-05,Time per Batch: 0.12 seconds     \n",
      "Epoch 10/500 finished. Total accuracy: 0.78151 AUROC: 0.6490224599838257 Time per Epoch: 44.64\n",
      "   10/  500 //   280/  281 | Loss: 0.3248,batch_accuracy:0.8333, last_total_accuracy: 0.7815126180648804, Maximum Accuracy 0.8403361439704895 last AUROC 0.6490224599838257 Max AUC 0.6558209657669067 lr:2.6911e-05,Time per Batch: 0.12 seconds     \n",
      "Epoch 11/500 finished. Total accuracy: 0.79832 AUROC: 0.6720553636550903 Time per Epoch: 42.163\n",
      "   11/  500 //   280/  281 | Loss: 0.1844,batch_accuracy:0.8333, last_total_accuracy: 0.7983193397521973, Maximum Accuracy 0.8403361439704895 last AUROC 0.6720553636550903 Max AUC 0.6720553636550903 lr:2.6588e-05,Time per Batch: 0.12 seconds     \n",
      "Epoch 12/500 finished. Total accuracy: 0.83193 AUROC: 0.6598789095878601 Time per Epoch: 43.993\n",
      "   12/  500 //   280/  281 | Loss: 0.07901,batch_accuracy:1.0, last_total_accuracy: 0.831932783126831, Maximum Accuracy 0.8403361439704895 last AUROC 0.6598789095878601 Max AUC 0.6720553636550903 lr:2.6269e-05,Time per Batch: 0.11 seconds      \n",
      "Epoch 13/500 finished. Total accuracy: 0.82353 AUROC: 0.6421564221382141 Time per Epoch: 42.556\n",
      "   13/  500 //   280/  281 | Loss: 0.5306,batch_accuracy:0.8333, last_total_accuracy: 0.8235294222831726, Maximum Accuracy 0.8403361439704895 last AUROC 0.6421564221382141 Max AUC 0.6720553636550903 lr:2.5954e-05,Time per Batch: 0.11 seconds     \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 14/500 finished. Total accuracy: 0.85714 AUROC: 0.6602286100387573 Time per Epoch: 39.73\n",
      "   14/  500 //   280/  281 | Loss: 0.199,batch_accuracy:0.8333, last_total_accuracy: 0.8571428656578064, Maximum Accuracy 0.8571428656578064 last AUROC 0.6602286100387573 Max AUC 0.6720553636550903 lr:2.5643e-05,Time per Batch: 0.12 seconds     \n",
      "Epoch 15/500 finished. Total accuracy: 0.83193 AUROC: 0.6649315357208252 Time per Epoch: 40.45\n",
      "   15/  500 //   280/  281 | Loss: 0.444,batch_accuracy:0.8333, last_total_accuracy: 0.831932783126831, Maximum Accuracy 0.8571428656578064 last AUROC 0.6649315357208252 Max AUC 0.6720553636550903 lr:2.5335e-05,Time per Batch: 0.11 seconds     \n",
      "Epoch 16/500 finished. Total accuracy: 0.81513 AUROC: 0.6691041588783264 Time per Epoch: 40.934\n",
      "   16/  500 //   280/  281 | Loss: 0.01498,batch_accuracy:1.0, last_total_accuracy: 0.8151260614395142, Maximum Accuracy 0.8571428656578064 last AUROC 0.6691041588783264 Max AUC 0.6720553636550903 lr:2.5031e-05,Time per Batch: 0.12 seconds       \n",
      "Epoch 17/500 finished. Total accuracy: 0.84034 AUROC: 0.6718324422836304 Time per Epoch: 42.969\n",
      "   17/  500 //   280/  281 | Loss: 0.07034,batch_accuracy:1.0, last_total_accuracy: 0.8403361439704895, Maximum Accuracy 0.8571428656578064 last AUROC 0.6718324422836304 Max AUC 0.6720553636550903 lr:2.473e-05,Time per Batch: 0.12 seconds      \n",
      "Epoch 18/500 finished. Total accuracy: 0.81513 AUROC: 0.6662971377372742 Time per Epoch: 44.251\n",
      "   18/  500 //   280/  281 | Loss: 0.06187,batch_accuracy:1.0, last_total_accuracy: 0.8151260614395142, Maximum Accuracy 0.8571428656578064 last AUROC 0.6662971377372742 Max AUC 0.6720553636550903 lr:2.4434e-05,Time per Batch: 0.11 seconds       \n",
      "Epoch 19/500 finished. Total accuracy: 0.84874 AUROC: 0.6657834649085999 Time per Epoch: 41.367\n",
      "   19/  500 //   280/  281 | Loss: 0.1423,batch_accuracy:0.8333, last_total_accuracy: 0.848739504814148, Maximum Accuracy 0.8571428656578064 last AUROC 0.6657834649085999 Max AUC 0.6720553636550903 lr:2.414e-05,Time per Batch: 0.11 seconds     \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 20/500 finished. Total accuracy: 0.88235 AUROC: 0.6687253713607788 Time per Epoch: 41.62\n",
      "   20/  500 //   280/  281 | Loss: 0.1207,batch_accuracy:1.0, last_total_accuracy: 0.8823529481887817, Maximum Accuracy 0.8823529481887817 last AUROC 0.6687253713607788 Max AUC 0.6720553636550903 lr:2.3851e-05,Time per Batch: 0.11 seconds        \n",
      "Epoch 21/500 finished. Total accuracy: 0.86555 AUROC: 0.6611225605010986 Time per Epoch: 44.512\n",
      "   21/  500 //   280/  281 | Loss: 0.2259,batch_accuracy:0.8333, last_total_accuracy: 0.8655462265014648, Maximum Accuracy 0.8823529481887817 last AUROC 0.6611225605010986 Max AUC 0.6720553636550903 lr:2.3565e-05,Time per Batch: 0.13 seconds     \n",
      "Epoch 22/500 finished. Total accuracy: 0.85714 AUROC: 0.6691806316375732 Time per Epoch: 41.672\n",
      "   22/  500 //   280/  281 | Loss: 0.9663,batch_accuracy:0.8333, last_total_accuracy: 0.8571428656578064, Maximum Accuracy 0.8823529481887817 last AUROC 0.6691806316375732 Max AUC 0.6720553636550903 lr:2.3282e-05,Time per Batch: 0.12 seconds     \n",
      "SAVING MODEL\n",
      "\n",
      "Epoch 23/500 finished. Total accuracy: 0.89076 AUROC: 0.6694715619087219 Time per Epoch: 44.418\n",
      "   23/  500 //   280/  281 | Loss: 0.3089,batch_accuracy:0.8333, last_total_accuracy: 0.8907563090324402, Maximum Accuracy 0.8907563090324402 last AUROC 0.6694715619087219 Max AUC 0.6720553636550903 lr:2.3002e-05,Time per Batch: 0.1 seconds      \n",
      "Epoch 24/500 finished. Total accuracy: 0.84034 AUROC: 0.6737124919891357 Time per Epoch: 40.106\n",
      "   24/  500 //   280/  281 | Loss: 0.364,batch_accuracy:0.6667, last_total_accuracy: 0.8403361439704895, Maximum Accuracy 0.8907563090324402 last AUROC 0.6737124919891357 Max AUC 0.6737124919891357 lr:2.2726e-05,Time per Batch: 0.1 seconds       \n",
      "Epoch 25/500 finished. Total accuracy: 0.86555 AUROC: 0.6647915244102478 Time per Epoch: 42.242\n",
      "   25/  500 //   280/  281 | Loss: 0.04164,batch_accuracy:1.0, last_total_accuracy: 0.8655462265014648, Maximum Accuracy 0.8907563090324402 last AUROC 0.6647915244102478 Max AUC 0.6737124919891357 lr:2.2454e-05,Time per Batch: 0.11 seconds       \n",
      "Epoch 26/500 finished. Total accuracy: 0.88235 AUROC: 0.6563305258750916 Time per Epoch: 41.399\n",
      "   26/  500 //   280/  281 | Loss: 0.001082,batch_accuracy:1.0, last_total_accuracy: 0.8823529481887817, Maximum Accuracy 0.8907563090324402 last AUROC 0.6563305258750916 Max AUC 0.6737124919891357 lr:2.2184e-05,Time per Batch: 0.11 seconds      \n",
      "Epoch 27/500 finished. Total accuracy: 0.84034 AUROC: 0.6567593216896057 Time per Epoch: 43.0\n",
      "   27/  500 //   280/  281 | Loss: 0.004989,batch_accuracy:1.0, last_total_accuracy: 0.8403361439704895, Maximum Accuracy 0.8907563090324402 last AUROC 0.6567593216896057 Max AUC 0.6737124919891357 lr:2.1918e-05,Time per Batch: 0.13 seconds      \n",
      "Epoch 28/500 finished. Total accuracy: 0.84874 AUROC: 0.663886547088623 Time per Epoch: 46.226\n",
      "   28/  500 //   280/  281 | Loss: 0.06756,batch_accuracy:1.0, last_total_accuracy: 0.848739504814148, Maximum Accuracy 0.8907563090324402 last AUROC 0.663886547088623 Max AUC 0.6737124919891357 lr:2.1655e-05,Time per Batch: 0.11 seconds       \n",
      "Epoch 29/500 finished. Total accuracy: 0.83193 AUROC: 0.6614060997962952 Time per Epoch: 46.786\n",
      "   29/  500 //   241/  281 | Loss: 0.163,batch_accuracy:0.875, last_total_accuracy: 0.831932783126831, Maximum Accuracy 0.8907563090324402 last AUROC 0.6614060997962952 Max AUC 0.6737124919891357 lr:2.1395e-05,Time per Batch: 0.15 seconds       \r"
     ]
    }
   ],
   "source": [
    "for i in range(trials):\n",
    "    wandb.init(project=\"simple_vision_transformer_forsmalldatasets_validation_test\")\n",
    "    config = wandb.config\n",
    "    config.run_name = wandb.run._run_id\n",
    "    config = wandb.config\n",
    "    config.epochs = 500\n",
    "    config.inx = 400\n",
    "    config.iny = 400\n",
    "    config.lr = startlr     \n",
    "    config.trial = i+1\n",
    "    config.total_trials = trials\n",
    "    config.best_model = OrderedDict()\n",
    "    config.start_time = datetime.datetime.now().isoformat()\n",
    "    config.savename = f\"best_model_state_dict_at_for{config.run_name}_stime_{config.start_time.replace(':', '-')}__acc_max_acc__auc_auc.pt\"\n",
    "    train_dl, test_dl = get_new_test_train(.05)\n",
    "    results.append(train_eval_model(wandb.config,train_dl,test_dl,nadam=True))\n",
    "    model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in results:\n",
    "    a.append([i[0].cpu().item(),i[1].cpu().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array(a)\n",
    "average_accuracy = np.average(results[:,0])\n",
    "average_auroc = np.average(results[:,1])\n",
    "print(average_accuracy,average_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(config.best_model,\n",
    "           f\"./saved_models/ViT/best_model_state_dict_at_for{config.run_name}_stime_{config.start_time.replace(':', '-')}_BEST_MODEL.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
