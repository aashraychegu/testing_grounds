{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoRe_Dataloader_ECSG import get_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Resize\n",
    "import math\n",
    "import torchinfo\n",
    "import time\n",
    "trainds = get_dataset(rsamp = .8)\n",
    "testds = get_dataset(rsamp = .2)\n",
    "train_dl = DataLoader(trainds,batch_size=6,shuffle = True,)\n",
    "test_dl = DataLoader(testds,batch_size=6,shuffle = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDB(nn.Module):\n",
    "    def __init__(self,inD,outD,dropout) -> None:\n",
    "        super().__init__()\n",
    "        inD = int(inD)\n",
    "        outD = int(outD)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(inD,outD),\n",
    "            nn.Dropout1d(p = dropout),\n",
    "            nn.BatchNorm1d(outD)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.seq(x)\n",
    "\n",
    "\n",
    "def conv_formula(d_in,kernel_size,padding = 0,dilation = 1,stride = 1):\n",
    "    return math.floor(((d_in+(2*padding)-(dilation*(kernel_size-1))-1)/stride)+1)\n",
    "\n",
    "\n",
    "class CDB(nn.Module):\n",
    "    def __init__(self,x_in,y_in,in_channels:int,out_channels:int,kernel_size:int,stride = 1,padding = 0,dilation = 1,dropout = .2,) -> None:\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential( # type: ignore\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels= out_channels,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride = stride,\n",
    "                      padding = padding,\n",
    "                      dilation=dilation\n",
    "                      ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Dropout2d(dropout),\n",
    "        )\n",
    "        self.x_out = conv_formula(x_in,kernel_size=kernel_size,stride=stride,padding=padding,dilation=dilation)\n",
    "        self.y_out = conv_formula(y_in,kernel_size=kernel_size,stride=stride,padding=padding,dilation=dilation)\n",
    "        self.out_channels = out_channels\n",
    "    def forward(self,x):\n",
    "        return self.seq(x)\n",
    "\n",
    "def new_CDB(x_in,y_in,in_channels:int = 1,out_channels:int = 1,kernel_size:int = 3,stride = 1,padding = 0,dilation = 1,dropout = .2,device = \"cpu\"):\n",
    "    cdb = CDB(x_in,y_in,in_channels,out_channels,kernel_size,stride,padding,dilation,dropout).to(device)\n",
    "    return cdb,cdb.x_out,cdb.y_out,cdb.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems: needs serious resizing to even work on this computer\n",
    "class simple_ann_cog(nn.Module):\n",
    "    def __init__(self,x,y,out_classes,device = \"cpu\") -> None:\n",
    "        super().__init__()\n",
    "        self.resize = Resize((200,200))\n",
    "        self.norm1 = nn.BatchNorm2d(1).to(device)\n",
    "        self.flatten = nn.Flatten().to(device)\n",
    "        self.l1 = LDB(x*y,4096,dropout = .2).to(device)\n",
    "        self.l2 = LDB(4096,4096/2,.2).to(device)\n",
    "        self.l3 = LDB(4096/2,out_classes,.2).to(device)\n",
    "        self.seq = nn.Sequential(self.l1,self.l2,self.l3)\n",
    "        self.softmax = nn.Softmax(-1).to(device)\n",
    "    def forward(self,x: torch.Tensor):\n",
    "        ix = self.resize(x)\n",
    "        xs = ix.shape\n",
    "        ix = ix.view((xs[0],1,xs[1],xs[2]))\n",
    "        ix = self.norm1(ix)\n",
    "        ix = self.flatten(ix)\n",
    "        ix = self.seq(ix)\n",
    "        return self.softmax(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashr\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "single_cnn_ann_cog                       [1, 19]                   --\n",
       "├─BatchNorm2d: 1-1                       [1, 1, 400, 400]          2\n",
       "├─CDB: 1-2                               [1, 12, 132, 132]         --\n",
       "│    └─Sequential: 2-1                   [1, 12, 132, 132]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 12, 132, 132]         600\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 12, 132, 132]         24\n",
       "│    │    └─Dropout2d: 3-3               [1, 12, 132, 132]         --\n",
       "├─CDB: 1-3                               [1, 32, 64, 64]           --\n",
       "│    └─Sequential: 2-2                   [1, 32, 64, 64]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 32, 64, 64]           9,632\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 32, 64, 64]           64\n",
       "│    │    └─Dropout2d: 3-6               [1, 32, 64, 64]           --\n",
       "├─Sequential: 1-4                        [1, 19]                   --\n",
       "│    └─Flatten: 2-3                      [1, 131072]               --\n",
       "│    └─LDB: 2-4                          [1, 2048]                 --\n",
       "│    │    └─Sequential: 3-7              [1, 2048]                 268,441,600\n",
       "│    └─LDB: 2-5                          [1, 1024]                 --\n",
       "│    │    └─Sequential: 3-8              [1, 1024]                 2,100,224\n",
       "│    └─LDB: 2-6                          [1, 19]                   --\n",
       "│    │    └─Sequential: 3-9              [1, 19]                   19,513\n",
       "├─Softmax: 1-5                           [1, 19]                   --\n",
       "==========================================================================================\n",
       "Total params: 270,571,659\n",
       "Trainable params: 270,571,659\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 320.47\n",
       "==========================================================================================\n",
       "Input size (MB): 0.64\n",
       "Forward/backward pass size (MB): 6.77\n",
       "Params size (MB): 1082.29\n",
       "Estimated Total Size (MB): 1089.70\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problems: needs serious resizing to even work on this computer\n",
    "class single_cnn_ann_cog(nn.Module):\n",
    "    def __init__(self,x,y,out_classes,device = \"cpu\") -> None:\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(1).to(device)\n",
    "        self.conv1,xout,yout,out_channels = new_CDB(x_in = 400, y_in = 400,in_channels = 1, out_channels = 12,kernel_size = 7,stride = 3,dropout=.2,device=device)\n",
    "        self.conv2,xout,yout,out_channels = new_CDB(x_in = xout,y_in=yout,in_channels=12,out_channels=32,kernel_size=5,stride=2,dropout=.2,device=device)\n",
    "        self.flatten = nn.Flatten().to(device)\n",
    "        self.l1 = LDB(xout*yout*out_channels,2048,dropout = .2).to(device)\n",
    "        self.l2 = LDB(2048,1024,.2).to(device)\n",
    "        self.l3 = LDB(1024,out_classes,.2).to(device)\n",
    "        self.seq = nn.Sequential(self.flatten,self.l1,self.l2,self.l3)\n",
    "        self.softmax = nn.Softmax(-1).to(device)\n",
    "        print(xout,yout,out_channels)\n",
    "    def forward(self,x: torch.Tensor):\n",
    "        xs = x.shape\n",
    "        ix = x.view((xs[0],1,xs[1],xs[2]))\n",
    "        ix = self.norm1(ix)\n",
    "        ix = self.conv1(ix)\n",
    "        ix = self.conv2(ix)\n",
    "        ix = self.seq(ix)\n",
    "        return self.softmax(ix)\n",
    "model = single_cnn_ann_cog(400,400,19,\"cuda:0\")\n",
    "torchinfo.summary(model,(1,400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics as metrics\n",
    "acc = metrics.Accuracy(task=\"multiclass\",num_classes=19).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maashraychegu\u001b[0m (\u001b[33malabs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\aashr\\Desktop\\research\\testing_grounds\\wandb\\run-20230224_131650-ymz9hh34</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alabs/testing_grounds/runs/ymz9hh34\" target=\"_blank\">super-music-12</a></strong> to <a href=\"https://wandb.ai/alabs/testing_grounds\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/alabs/testing_grounds\" target=\"_blank\">https://wandb.ai/alabs/testing_grounds</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/alabs/testing_grounds/runs/ymz9hh34\" target=\"_blank\">https://wandb.ai/alabs/testing_grounds/runs/ymz9hh34</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(\"sdisc-2d-2convlayer\")\n",
    "config = wandb.config\n",
    "config.epochs = 100\n",
    "config.inx = 400\n",
    "config.iny = 400\n",
    "config.lr = 1e-5\n",
    "optimizer = optim.Adam(params = model.parameters(),lr = config.lr)\n",
    "lossfn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 {'loss': 2.9439258575439453, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 1 {'loss': 2.943370819091797, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 2 {'loss': 2.929135322570801, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 3 {'loss': 2.893190383911133, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 4 {'loss': 2.90382981300354, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 5 {'loss': 2.9225337505340576, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 6 {'loss': 2.9279708862304688, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 7 {'loss': 2.929328203201294, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 8 {'loss': 2.9676802158355713, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 9 {'loss': 2.937041997909546, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 10 {'loss': 2.9435269832611084, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 11 {'loss': 2.9369847774505615, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 12 {'loss': 2.9590837955474854, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 13 {'loss': 2.931403160095215, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 14 {'loss': 2.964574098587036, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 15 {'loss': 2.9586286544799805, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 16 {'loss': 2.960299253463745, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 17 {'loss': 2.911883592605591, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 18 {'loss': 2.9019572734832764, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 19 {'loss': 2.895254135131836, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 20 {'loss': 2.9216887950897217, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 21 {'loss': 2.9328718185424805, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 22 {'loss': 2.9169647693634033, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 23 {'loss': 2.8990890979766846, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 24 {'loss': 2.9101474285125732, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 25 {'loss': 2.908825635910034, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 26 {'loss': 2.91100811958313, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 27 {'loss': 2.911651611328125, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 28 {'loss': 2.9100863933563232, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 29 {'loss': 2.9004642963409424, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 30 {'loss': 2.8875696659088135, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 31 {'loss': 2.894719123840332, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 32 {'loss': 2.9440205097198486, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 33 {'loss': 2.921525001525879, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 34 {'loss': 2.9106338024139404, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 35 {'loss': 2.884735107421875, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 36 {'loss': 2.8914308547973633, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 37 {'loss': 2.9103524684906006, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 38 {'loss': 2.903223752975464, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 39 {'loss': 2.902078866958618, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 40 {'loss': 2.943716049194336, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 41 {'loss': 2.9220659732818604, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 42 {'loss': 2.9553918838500977, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 43 {'loss': 2.931978940963745, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 44 {'loss': 2.894543409347534, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 45 {'loss': 2.9616775512695312, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 46 {'loss': 2.9000751972198486, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 47 {'loss': 2.9077670574188232, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 48 {'loss': 2.8177003860473633, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 49 {'loss': 2.929372787475586, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 50 {'loss': 2.955598831176758, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 51 {'loss': 2.9183061122894287, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 52 {'loss': 2.9031975269317627, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 53 {'loss': 2.890442132949829, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 54 {'loss': 2.90016770362854, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 55 {'loss': 2.9230260848999023, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 56 {'loss': 2.934044599533081, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 57 {'loss': 2.8844118118286133, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 58 {'loss': 2.893054246902466, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 59 {'loss': 2.938070058822632, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 60 {'loss': 2.9589481353759766, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 61 {'loss': 2.9312171936035156, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 62 {'loss': 2.888162612915039, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 63 {'loss': 2.8868179321289062, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 64 {'loss': 2.8866984844207764, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 65 {'loss': 2.8793888092041016, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 66 {'loss': 2.9464359283447266, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 67 {'loss': 2.921369791030884, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 68 {'loss': 2.8990352153778076, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 69 {'loss': 2.898247003555298, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 70 {'loss': 2.9586594104766846, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 71 {'loss': 2.9275033473968506, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 72 {'loss': 2.9179041385650635, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 73 {'loss': 2.910428047180176, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 74 {'loss': 2.906919479370117, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 75 {'loss': 2.926645517349243, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 76 {'loss': 2.862459897994995, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 77 {'loss': 2.8143932819366455, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 78 {'loss': 2.950319528579712, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 79 {'loss': 2.8967154026031494, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 80 {'loss': 2.8932104110717773, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 81 {'loss': 2.901224136352539, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 82 {'loss': 2.829291582107544, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 83 {'loss': 2.8875362873077393, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 84 {'loss': 2.899951696395874, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 85 {'loss': 2.8661890029907227, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 86 {'loss': 2.958743095397949, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 87 {'loss': 2.8528096675872803, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 88 {'loss': 2.8725225925445557, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 89 {'loss': 2.9498157501220703, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 90 {'loss': 2.870096206665039, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 91 {'loss': 2.9149348735809326, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 92 {'loss': 2.865797281265259, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 93 {'loss': 2.895051956176758, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 94 {'loss': 2.9211466312408447, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 95 {'loss': 2.8693549633026123, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 96 {'loss': 2.8475990295410156, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 97 {'loss': 2.9039723873138428, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 98 {'loss': 2.8504698276519775, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 99 {'loss': 2.8739490509033203, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 100 {'loss': 2.902348518371582, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 101 {'loss': 2.952901840209961, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 102 {'loss': 2.8389275074005127, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 103 {'loss': 2.9254872798919678, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 104 {'loss': 2.9588019847869873, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 105 {'loss': 2.9499197006225586, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 106 {'loss': 2.851473569869995, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 107 {'loss': 2.9345052242279053, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 108 {'loss': 2.9179601669311523, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 109 {'loss': 2.7910215854644775, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 110 {'loss': 2.971095323562622, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 111 {'loss': 2.863694190979004, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 112 {'loss': 2.849442720413208, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 113 {'loss': 2.8770086765289307, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 114 {'loss': 2.9497222900390625, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 115 {'loss': 2.898102045059204, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 116 {'loss': 2.8449742794036865, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 117 {'loss': 2.9721128940582275, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 118 {'loss': 2.966740369796753, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 119 {'loss': 2.933872938156128, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 120 {'loss': 2.8390843868255615, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 121 {'loss': 2.834357500076294, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 122 {'loss': 2.912520408630371, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 123 {'loss': 2.8766448497772217, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 124 {'loss': 2.8008711338043213, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 125 {'loss': 2.851644515991211, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 126 {'loss': 2.8201780319213867, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 127 {'loss': 2.9570209980010986, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 128 {'loss': 2.882520914077759, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 129 {'loss': 2.867465019226074, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 130 {'loss': 2.8465778827667236, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 131 {'loss': 2.8554601669311523, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 132 {'loss': 2.9062607288360596, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 133 {'loss': 2.868765115737915, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 134 {'loss': 2.9608993530273438, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 135 {'loss': 2.9227349758148193, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 136 {'loss': 2.836226224899292, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 137 {'loss': 2.844078302383423, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 138 {'loss': 2.90252423286438, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 139 {'loss': 2.8327715396881104, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 140 {'loss': 2.9189653396606445, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 141 {'loss': 2.859238862991333, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 142 {'loss': 2.8272769451141357, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 143 {'loss': 2.9111602306365967, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 144 {'loss': 2.93780779838562, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 145 {'loss': 2.8774776458740234, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 146 {'loss': 2.8974075317382812, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 147 {'loss': 2.904475450515747, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 148 {'loss': 2.8551080226898193, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 149 {'loss': 2.8251993656158447, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 150 {'loss': 2.8889005184173584, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 151 {'loss': 2.958242416381836, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 152 {'loss': 2.848564863204956, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 153 {'loss': 2.8659896850585938, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 154 {'loss': 2.9023263454437256, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 155 {'loss': 2.933685302734375, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 156 {'loss': 2.8917293548583984, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 157 {'loss': 2.86496639251709, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 158 {'loss': 2.8487741947174072, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 159 {'loss': 2.9466552734375, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 160 {'loss': 2.826220750808716, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 161 {'loss': 2.832636833190918, 'batch_accuracy': tensor(0.8333, device='cuda:0')}\n",
      "1 162 {'loss': 2.896721839904785, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 163 {'loss': 2.838629961013794, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 164 {'loss': 2.8374128341674805, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 165 {'loss': 2.8132009506225586, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 166 {'loss': 2.830061912536621, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 167 {'loss': 2.9420528411865234, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 168 {'loss': 2.8271453380584717, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 169 {'loss': 2.949888229370117, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 170 {'loss': 2.921755790710449, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 171 {'loss': 2.931752920150757, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 172 {'loss': 2.9625511169433594, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 173 {'loss': 2.8271706104278564, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 174 {'loss': 2.9435713291168213, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 175 {'loss': 2.935967206954956, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 176 {'loss': 2.901373863220215, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 177 {'loss': 2.805659055709839, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 178 {'loss': 2.8895013332366943, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 179 {'loss': 2.9497900009155273, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 180 {'loss': 2.929196357727051, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 181 {'loss': 2.8510377407073975, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 182 {'loss': 2.8845741748809814, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 183 {'loss': 2.8319644927978516, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 184 {'loss': 2.7762861251831055, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 185 {'loss': 2.9181602001190186, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 186 {'loss': 2.865778684616089, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 187 {'loss': 2.8704748153686523, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 188 {'loss': 2.835895538330078, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 189 {'loss': 2.9300692081451416, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 190 {'loss': 2.871525526046753, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 191 {'loss': 2.7779064178466797, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 192 {'loss': 2.827174425125122, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 193 {'loss': 2.853746175765991, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 194 {'loss': 2.839465379714966, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 195 {'loss': 2.7997188568115234, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 196 {'loss': 2.8662469387054443, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 197 {'loss': 2.8432424068450928, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 198 {'loss': 2.9050912857055664, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 199 {'loss': 2.857584238052368, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 200 {'loss': 2.9101359844207764, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 201 {'loss': 2.8932902812957764, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 202 {'loss': 2.766599655151367, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 203 {'loss': 2.927884101867676, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 204 {'loss': 2.8017613887786865, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 205 {'loss': 2.792015790939331, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 206 {'loss': 2.9441325664520264, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 207 {'loss': 2.82830810546875, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 208 {'loss': 2.945404291152954, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 209 {'loss': 2.804734230041504, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 210 {'loss': 2.8013508319854736, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 211 {'loss': 2.8290178775787354, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 212 {'loss': 2.796095609664917, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 213 {'loss': 2.8116464614868164, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 214 {'loss': 2.8764631748199463, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 215 {'loss': 2.8730623722076416, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 216 {'loss': 2.9283316135406494, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 217 {'loss': 2.815566301345825, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 218 {'loss': 2.974154472351074, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 219 {'loss': 2.863172769546509, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 220 {'loss': 2.8960297107696533, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 221 {'loss': 2.9277305603027344, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 222 {'loss': 2.8598413467407227, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 223 {'loss': 2.9220378398895264, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 224 {'loss': 2.918026924133301, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 225 {'loss': 2.9397451877593994, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 226 {'loss': 2.804941415786743, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 227 {'loss': 2.8214309215545654, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 228 {'loss': 2.957270622253418, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 229 {'loss': 2.939295530319214, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 230 {'loss': 2.971940279006958, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 231 {'loss': 2.9382431507110596, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 232 {'loss': 2.891482353210449, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 233 {'loss': 2.8223049640655518, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 234 {'loss': 2.829155206680298, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 235 {'loss': 2.890275239944458, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 236 {'loss': 2.900751829147339, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 237 {'loss': 2.915914535522461, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 238 {'loss': 2.779313087463379, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 239 {'loss': 2.7870101928710938, 'batch_accuracy': tensor(0.8333, device='cuda:0')}\n",
      "1 240 {'loss': 2.9729840755462646, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 241 {'loss': 2.9415266513824463, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 242 {'loss': 2.8119924068450928, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 243 {'loss': 2.8129637241363525, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 244 {'loss': 2.8978288173675537, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 245 {'loss': 2.9295289516448975, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 246 {'loss': 2.805760145187378, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 247 {'loss': 2.8304646015167236, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 248 {'loss': 2.7943274974823, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 249 {'loss': 2.8709847927093506, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 250 {'loss': 2.8210084438323975, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 251 {'loss': 2.8806374073028564, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 252 {'loss': 2.931586980819702, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 253 {'loss': 2.9373176097869873, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 254 {'loss': 2.892805814743042, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 255 {'loss': 2.835644483566284, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 256 {'loss': 2.9709882736206055, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 257 {'loss': 2.821549654006958, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 258 {'loss': 2.82533860206604, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 259 {'loss': 2.843769073486328, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 260 {'loss': 2.9000377655029297, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 261 {'loss': 2.823018789291382, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 262 {'loss': 2.803288698196411, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 263 {'loss': 2.9297094345092773, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 264 {'loss': 2.9386110305786133, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 265 {'loss': 2.93894100189209, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 266 {'loss': 2.8265349864959717, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 267 {'loss': 2.903964042663574, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 268 {'loss': 2.8950881958007812, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 269 {'loss': 2.9509074687957764, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 270 {'loss': 2.797940492630005, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 271 {'loss': 2.8336904048919678, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 272 {'loss': 2.827285051345825, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 273 {'loss': 2.959584951400757, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 274 {'loss': 2.83412766456604, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 275 {'loss': 2.8524577617645264, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 276 {'loss': 2.8687832355499268, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 277 {'loss': 2.8776023387908936, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 278 {'loss': 2.8258321285247803, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 279 {'loss': 2.8268940448760986, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 280 {'loss': 2.857762098312378, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 281 {'loss': 2.9580090045928955, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 282 {'loss': 2.892491579055786, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 283 {'loss': 2.9338321685791016, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 284 {'loss': 2.826963424682617, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 285 {'loss': 2.906791925430298, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 286 {'loss': 2.890183687210083, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 287 {'loss': 2.784719467163086, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 288 {'loss': 2.792253255844116, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 289 {'loss': 2.8137362003326416, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 290 {'loss': 2.778254747390747, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 291 {'loss': 2.882660150527954, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 292 {'loss': 2.944035768508911, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 293 {'loss': 2.820296287536621, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 294 {'loss': 2.9057600498199463, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 295 {'loss': 2.7871828079223633, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 296 {'loss': 2.848158836364746, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 297 {'loss': 2.9617440700531006, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 298 {'loss': 2.8053205013275146, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 299 {'loss': 2.8575503826141357, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 300 {'loss': 2.9541330337524414, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 301 {'loss': 2.871143341064453, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 302 {'loss': 2.805363416671753, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 303 {'loss': 2.8001997470855713, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 304 {'loss': 2.918020248413086, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 305 {'loss': 2.9162368774414062, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 306 {'loss': 2.860714912414551, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 307 {'loss': 2.804877519607544, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "1 308 {'loss': 2.7952191829681396, 'batch_accuracy': tensor(0.8333, device='cuda:0')}\n",
      "1 309 {'loss': 2.964055061340332, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "1 310 {'loss': 2.7613017559051514, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 311 {'loss': 2.9270098209381104, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "1 312 {'loss': 2.885298013687134, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 313 {'loss': 2.8244407176971436, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "1 314 {'loss': 2.7941150665283203, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "1 315 {'loss': 2.8230345249176025, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 0 {'loss': 2.8892440795898438, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 1 {'loss': 2.8777740001678467, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 2 {'loss': 2.8265163898468018, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 3 {'loss': 2.955888032913208, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 4 {'loss': 2.847698450088501, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 5 {'loss': 2.789438486099243, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 6 {'loss': 2.8264119625091553, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 7 {'loss': 2.9362049102783203, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 8 {'loss': 2.7576496601104736, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "2 9 {'loss': 2.902259588241577, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 10 {'loss': 2.9299018383026123, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 11 {'loss': 2.9647858142852783, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 12 {'loss': 2.787122964859009, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 13 {'loss': 2.9596974849700928, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 14 {'loss': 2.7626636028289795, 'batch_accuracy': tensor(0.8333, device='cuda:0')}\n",
      "2 15 {'loss': 2.9634625911712646, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 16 {'loss': 2.813314437866211, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 17 {'loss': 2.8985249996185303, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 18 {'loss': 2.830324172973633, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 19 {'loss': 2.780778169631958, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 20 {'loss': 2.883168935775757, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 21 {'loss': 2.8450403213500977, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 22 {'loss': 2.785027265548706, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 23 {'loss': 2.8143138885498047, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 24 {'loss': 2.8663933277130127, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 25 {'loss': 2.7003517150878906, 'batch_accuracy': tensor(0.8333, device='cuda:0')}\n",
      "2 26 {'loss': 2.782335042953491, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "2 27 {'loss': 2.889416456222534, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 28 {'loss': 2.9316165447235107, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 29 {'loss': 2.7993545532226562, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 30 {'loss': 2.861861228942871, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 31 {'loss': 2.898608922958374, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 32 {'loss': 2.940518617630005, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 33 {'loss': 2.816608190536499, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 34 {'loss': 2.8747856616973877, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 35 {'loss': 2.793269157409668, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 36 {'loss': 2.941671133041382, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 37 {'loss': 2.85037899017334, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "2 38 {'loss': 2.792694091796875, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 39 {'loss': 2.912590265274048, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 40 {'loss': 2.793654680252075, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 41 {'loss': 2.73239803314209, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "2 42 {'loss': 2.9296867847442627, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 43 {'loss': 2.7503814697265625, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "2 44 {'loss': 2.9185283184051514, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 45 {'loss': 2.962721586227417, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 46 {'loss': 2.843942642211914, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "2 47 {'loss': 2.8230206966400146, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 48 {'loss': 2.8954741954803467, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 49 {'loss': 2.9090538024902344, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 50 {'loss': 2.812912702560425, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 51 {'loss': 2.9282286167144775, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 52 {'loss': 2.8918304443359375, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 53 {'loss': 2.8793678283691406, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 54 {'loss': 2.8751089572906494, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 55 {'loss': 2.942244291305542, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 56 {'loss': 2.9431686401367188, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 57 {'loss': 2.9331846237182617, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 58 {'loss': 2.8517396450042725, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 59 {'loss': 2.93963623046875, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 60 {'loss': 2.8132359981536865, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 61 {'loss': 2.8108303546905518, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 62 {'loss': 2.8097751140594482, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 63 {'loss': 2.873520851135254, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 64 {'loss': 2.8106682300567627, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 65 {'loss': 2.9543979167938232, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 66 {'loss': 2.777698278427124, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 67 {'loss': 2.9276010990142822, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 68 {'loss': 2.812716245651245, 'batch_accuracy': tensor(0.6667, device='cuda:0')}\n",
      "2 69 {'loss': 2.76265811920166, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 70 {'loss': 2.8392112255096436, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 71 {'loss': 2.913731575012207, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 72 {'loss': 2.807432174682617, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 73 {'loss': 2.917562246322632, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 74 {'loss': 2.803009271621704, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 75 {'loss': 2.8587560653686523, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 76 {'loss': 2.8524160385131836, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 77 {'loss': 2.728438377380371, 'batch_accuracy': tensor(1., device='cuda:0')}\n",
      "2 78 {'loss': 2.873302459716797, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 79 {'loss': 2.708117723464966, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 80 {'loss': 2.9069135189056396, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 81 {'loss': 2.956331491470337, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 82 {'loss': 2.8078174591064453, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 83 {'loss': 2.9604837894439697, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 84 {'loss': 2.805262327194214, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 85 {'loss': 2.771430253982544, 'batch_accuracy': tensor(0.5000, device='cuda:0')}\n",
      "2 86 {'loss': 2.9028570652008057, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 87 {'loss': 2.919544219970703, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 88 {'loss': 2.9283933639526367, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 89 {'loss': 2.9564409255981445, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 90 {'loss': 2.9714114665985107, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 91 {'loss': 2.9388883113861084, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 92 {'loss': 2.9193859100341797, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 93 {'loss': 2.8055427074432373, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 94 {'loss': 2.9103682041168213, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 95 {'loss': 2.8109333515167236, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 96 {'loss': 2.944101095199585, 'batch_accuracy': tensor(0.1667, device='cuda:0')}\n",
      "2 97 {'loss': 2.8226120471954346, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n",
      "2 98 {'loss': 2.9482476711273193, 'batch_accuracy': tensor(0., device='cuda:0')}\n",
      "2 99 {'loss': 2.789491653442383, 'batch_accuracy': tensor(0.3333, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,config.epochs+2):\n",
    "    ldl = len(train_dl)\n",
    "    for batch,(sg,params) in enumerate(train_dl):\n",
    "        sg = sg.to(\"cuda:0\").to(torch.float)\n",
    "        params = params[:,0].to(\"cuda:0\").to(torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sg)\n",
    "        loss = lossfn(outputs,params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\"loss\":loss.item(),\"batch_accuracy\":acc(outputs,params)})\n",
    "        print(epoch,ldl,batch,{\"loss\":loss.item(),\"batch_accuracy\":acc(outputs,params)},time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = new_accuracy(model,test_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
