{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoRe_Dataloader_ECSG import get_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "trainds = get_dataset(rsamp = .8)\n",
    "testds = get_dataset(rsamp = .2)\n",
    "train_dl = DataLoader(trainds,batch_size=8,shuffle = True,)\n",
    "test_ds = DataLoader(testds,batch_size=8,shuffle = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDB(nn.Module):\n",
    "    def __init__(self,inD,outD,dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(inD,outD),\n",
    "            nn.Dropout1d(p = dropout),\n",
    "            nn.BatchNorm1d(outD)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.seq(x)\n",
    "l = LDB(10,10,.5)\n",
    "# l(torch.randn(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cog(nn.Module):\n",
    "    def __init__(self,x,y,out_classes,device = \"cpu\") -> None:\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(1).to(device)\n",
    "        self.flatten = nn.Flatten().to(device)\n",
    "        self.l1 = LDB(x*y,4096,dropout = .2).to(device)\n",
    "        self.l2 = LDB(4096,4096/2,.2).to(device)\n",
    "        self.l3 = LDB(4096/2,out_classes,.2).to(device)\n",
    "        self.seq = nn.Sequential(self.l1,self.l2,self.l3)\n",
    "        self.softmax = nn.Softmax(-1).to(device)\n",
    "    def forward(self,x: torch.Tensor):\n",
    "        xs = x.shape\n",
    "        ix = x.view((xs[0],1,xs[1],xs[2]))\n",
    "        ix = self.norm1(ix)\n",
    "        ix = self.flatten(ix)\n",
    "        ix = self.seq(ix)\n",
    "        return self.softmax(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cog(400,400,19,\"cuda:0\")\n",
    "ti = torch.randn(16,400,400).to(\"cuda:0\")\n",
    "# model(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "torchinfo.summary(model,(1,400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(\"sdisc-2d\")\n",
    "config = wandb.config\n",
    "config.epochs = 100\n",
    "config.inx = 400\n",
    "config.iny = 400\n",
    "optimizer = optim.Adam(params = model.parameters(),lr = 1e-5)\n",
    "lossfn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1,config.epochs+2):\n",
    "    ldl = len(train_dl)\n",
    "    for batch,(sg,params) in enumerate(train_dl):\n",
    "        sg = sg.to(\"cuda:0\").to(torch.float)\n",
    "        params = params[:,0].to(\"cuda:0\").to(torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sg)\n",
    "        loss = lossfn(outputs,params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(epoch,batch,loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
