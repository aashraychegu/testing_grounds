{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.k = k # Dimension of time2vec vector\n",
    "        self.w = nn.Parameter(torch.randn(k)) # Learnable parameters for linear part\n",
    "        self.b = nn.Parameter(torch.randn(k)) # Learnable parameters for linear part\n",
    "        self.w_sin = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "        self.b_sin = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "        self.w_cos = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "        self.b_cos = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1) # Add extra dimension for vectorization\n",
    "        linear = self.w * x + self.b # Linear transformation (k features)\n",
    "        sin_trans = torch.sin(self.w_sin * x + self.b_sin) # Periodic transformation (k features)\n",
    "        cos_trans = torch.cos(self.w_cos * x + self.b_cos) # Periodic transformation (k features)\n",
    "        return torch.cat([linear, sin_trans, cos_trans],-1) # Concatenate along last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoRe_Dataloader6 import dataloader   \n",
    "train_dl, test_dl = dataloader,dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "n_features = 8 # Number of features per time step\n",
    "n_layers = 4 # Number of transformer encoder layers\n",
    "n_heads = 8 # Number of attention heads per layer\n",
    "hidden_size = 1024 # Size of hidden state in sublayers \n",
    "dropout_rate = 0.4 # Dropout rate for regularization\n",
    "n_classes = 19\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of SelfAttentionPooling\n",
    "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
    "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttentionPooling, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "        self.softmax = nn.functional.softmax\n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "\n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "\n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "\n",
    "        att_w = self.softmax(self.W(batch_rep).squeeze(-1), -1).unsqueeze(-1)\n",
    "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
    "\n",
    "        return utter_rep\n",
    "\n",
    "# Define classifier model \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self , n_layers , n_features , n_heads , hidden_size , dropout_rate , n_classes):\n",
    "        super(Classifier , self).__init__()\n",
    "        self.t2v = Time2Vec(n_features)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(n_features*3,n_heads,hidden_size,dropout_rate),n_layers) # Transformer encoder layer \n",
    "        self.pooling= SelfAttentionPooling(n_features*3) # Global average pooling layer \n",
    "        self.linear= nn.Linear(n_features*3,n_classes) # Linear layer with softmax activation \n",
    "\n",
    "    def forward(self,x):\n",
    "        # x shape: (batch_size ,seq_len ,n_features)\n",
    "        x = self.t2v(x)\n",
    "        # print(x.shape)\n",
    "        x= self.transformer_encoder(x) # Apply transformer encoder on x \n",
    "        # x= x.permute(1,0 ,2) # Permute x to match expected shape for pooling (batch_size ,n_features ,seq_len)\n",
    "        x= self.pooling(x) # Apply pooling on x \n",
    "        # x= x.squeeze(-1) # Remove last dimension \n",
    "        # print(x.shape)\n",
    "        x= self.linear(x) # Apply linear layer on x \n",
    "        return nn.functional.softmax(x,dim=-1) #Return class probabilities (batch_size ,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(\n",
    "    n_layers=n_layers,\n",
    "    n_features=n_features,\n",
    "    n_heads=n_heads,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout_rate=dropout_rate,\n",
    "    n_classes=n_classes,\n",
    ").to(\"cuda:0\")\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor(\n",
    "        [\n",
    "            1,\n",
    "            1.0000,\n",
    "            0.5,\n",
    "            1.0000,\n",
    "            1,\n",
    "            1,\n",
    "            1,\n",
    "            1.0000,\n",
    "            1,\n",
    "            1.0000,\n",
    "            0.5,\n",
    "            1,\n",
    "            1,\n",
    "            1,\n",
    "            0.5,\n",
    "            1,\n",
    "            0.1,\n",
    "            1.0000,\n",
    "            1,\n",
    "        ]\n",
    "    )\n",
    ").to(\n",
    "    \"cuda:0\"\n",
    ")  # Cross entropy loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)  # Adam optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 15, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maashraychegu\u001b[0m (\u001b[33malabs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\aashr\\Desktop\\research\\testing_grounds\\wandb\\run-20230219_153901-hvhe5xy4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alabs/testing_grounds/runs/hvhe5xy4\" target=\"_blank\">soft-forest-1</a></strong> to <a href=\"https://wandb.ai/alabs/testing_grounds\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/alabs/testing_grounds\" target=\"_blank\">https://wandb.ai/alabs/testing_grounds</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/alabs/testing_grounds/runs/hvhe5xy4\" target=\"_blank\">https://wandb.ai/alabs/testing_grounds/runs/hvhe5xy4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 38/71 : Loss 2.8646452426910495\r"
     ]
    }
   ],
   "source": [
    "wandb.init(\"transformer-test-c2\")\n",
    "n_epochs = 1000\n",
    "# Train the model on the training data \n",
    "for epoch in range(n_epochs): #Iterate over epochs \n",
    "    epochloss = 0\n",
    "    ldl = len(train_dl)\n",
    "    for bnum,(batch_x,batch_y) in enumerate(train_dl): #Iterate over batches \n",
    "        # print(batch_x.shape,batch_y.shape)\n",
    "        batch_x = batch_x.to(\"cuda:0\").to(torch.float)\n",
    "        batch_y = batch_y.to(\"cuda:0\").to(torch.long)[:,0]\n",
    "        optimizer.zero_grad() #Clear previous gradients \n",
    "        \n",
    "        output=model(batch_x).to(torch.float) #Get model output for current batch (batch_step ,n_classes)\n",
    "        # print(output.shape)\n",
    "        loss=loss_fn(output.to(torch.float),batch_y) #Compute loss for current batch \n",
    "        loss.backward() #Backpropagate loss \n",
    "        optimizer.step() #Update parameters \n",
    "        \n",
    "        print(f\"Epoch {epoch} Batch {bnum}/{ldl} : Loss {loss.item()}\",end = \"\\r\",flush=True) #Print epoch and loss \n",
    "        epochloss+=loss.item()\n",
    "        wandb.log({\"batchloss\":loss.item()})\n",
    "    if epoch > 20 and epoch < 300: scheduler.step() \n",
    "    epochloss/=ldl\n",
    "\n",
    "    print(f\"\\nAverage epoch loss: {epochloss} with Learning rate {scheduler.get_last_lr()}\")\n",
    "    print(\"Evaluating with\",end = \" \")\n",
    "    with torch.no_grad(): # Disable gradient computation \n",
    "        accsum = []\n",
    "        for bnum,(batch_x,batch_y) in enumerate(train_dl):\n",
    "            batch_x = batch_x.to(\"cuda:0\")\n",
    "            batch_y = batch_y.to(\"cuda:0\").to(torch.long)[:,0]\n",
    "            output=model(batch_x) #Get model output for validation data (val_batch_size ,n_classes)\n",
    "            pred=torch.argmax(output,dim=-1) # Get predicted classes (val_batch_size)\n",
    "            accsum.append((pred==batch_y).float()) # Compute accuracy (%)\n",
    "            # print(inp.shape, output.shape, pred.shape, rout.shape,acc)\n",
    "        # print(f\"Accuracy on validation data : {acc}%\")# Print accuracy\n",
    "        equals= torch.cat(accsum)\n",
    "        means = torch.mean(equals)\n",
    "        print(\"mean total accuracy:\", means.item()*100,\"%\\n\",\"-=<{|}>=-\"*8,'\\n')\n",
    "        wandb.log({\"accuracy\":means.item()})\n",
    "    \n",
    "    # Evaluate the model on the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # Disable gradient computation \n",
    "    accsum = []\n",
    "    for bnum,(batch_x,batch_y) in enumerate(test_dl):\n",
    "        batch_x = batch_x.to(\"cuda:0\")\n",
    "        batch_y = batch_y.to(\"cuda:0\")\n",
    "        print(bnum)\n",
    "        output=model(batch_x) #Get model output for validation data (val_batch_size ,n_classes)\n",
    "        pred=torch.argmax(output,dim=-1) # Get predicted classes (val_batch_size)\n",
    "        accsum.append((pred==batch_y).float()) # Compute accuracy (%)\n",
    "        # print(inp.shape, output.shape, pred.shape, rout.shape,acc)\n",
    "    # print(f\"Accuracy on validation data : {acc}%\")# Print accuracy\n",
    "    equals= torch.cat(accsum)\n",
    "    means = torch.mean(equals)\n",
    "    print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=500_000)\n",
    "print(equals)\n",
    "torch.set_printoptions(profile='default')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
