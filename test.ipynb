{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.k = k # Dimension of time2vec vector\n",
    "        self.w = nn.Parameter(torch.randn(k)) # Learnable parameters for linear part\n",
    "        self.b = nn.Parameter(torch.randn(k)) # Learnable parameters for linear part\n",
    "        self.w_sin = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "        self.b_sin = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "        self.w_cos = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "        self.b_cos = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1) # Add extra dimension for vectorization\n",
    "        linear = self.w * x + self.b # Linear transformation (k features)\n",
    "        sin_trans = torch.sin(self.w_sin * x + self.b_sin) # Periodic transformation (k features)\n",
    "        cos_trans = torch.cos(self.w_cos * x + self.b_cos) # Periodic transformation (k features)\n",
    "        return torch.cat([linear, sin_trans, cos_trans,sin_trans+cos_trans+linear],-1) # Concatenate along last dimension\n",
    "t2v = Time2Vec(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Define hyperparameters\n",
    "# batch_size = 32 # Number of samples per batch\n",
    "# seq_len = 100 # Length of each time series\n",
    "# in_features = 1\n",
    "# n_classes = 20 # Number of classes to predict\n",
    "\n",
    "# # Create synthetic dataset (X: input features, y: labels)\n",
    "# # X = np.random.randn(100,batch_size * seq_len * in_features).reshape(100, batch_size , seq_len)\n",
    "# # y = np.random.randint(0, n_classes , size=(100,batch_size))\n",
    "# def gen_ds(samples = 32_000,name = \"dl\",batch_size = 32):\n",
    "#     assert samples%n_classes == 0\n",
    "#     xcont = []\n",
    "#     ycont = []\n",
    "#     for i in range(n_classes):\n",
    "#         cnumb = int(samples/n_classes)\n",
    "#         ox = np.random.randn(samples,seq_len*in_features).reshape(samples,seq_len)\n",
    "#         lx = ox + i\n",
    "#         ly = np.full((samples),i)\n",
    "#         print()\n",
    "#         xcont.append(lx)\n",
    "#         ycont.append(ly)\n",
    "#         print(np.mean(ox),np.mean(lx),np.mean(ly),i)\n",
    "\n",
    "\n",
    "#     X = np.concatenate(xcont)\n",
    "#     Y = np.concatenate(ycont)\n",
    "\n",
    "#     # Convert numpy arrays to tensors \n",
    "#     X = torch.from_numpy(X).float()\n",
    "#     y = torch.from_numpy(Y).long()\n",
    "\n",
    "#     class tds(Dataset):\n",
    "#         def __init__(self,x,y,name) -> None:\n",
    "#             super().__init__()\n",
    "#             self.name = name\n",
    "#             self.X = x\n",
    "#             self.Y = y\n",
    "#             self.pspace = len(self.X)\n",
    "#         def __len__(self):\n",
    "#             return self.pspace\n",
    "#         def __getitem__(self, index):\n",
    "#             return X[index],Y[index]\n",
    "#     ds = tds(X,y,name)\n",
    "#     dl = DataLoader(ds,batch_size=batch_size,shuffle=True)\n",
    "#     return dl\n",
    "# # train_dl = gen_ds(samples = 800*20,name = \"train_dl\")\n",
    "# # test_dl = gen_ds(1000,name = \"test_dl\",batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoRe_Dataloader import dataloader   \n",
    "train_dl, test_dl = dataloader,dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "n_features = 16 # Number of features per time step\n",
    "n_layers = 3 # Number of transformer encoder layers\n",
    "n_heads = 8 # Number of attention heads per layer\n",
    "hidden_size = 1024 # Size of hidden state in sublayers \n",
    "dropout_rate = 0.2 # Dropout rate for regularization\n",
    "n_classes = 19\n",
    "\n",
    "# Define classifier model \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self , n_layers , n_features , n_heads , hidden_size , dropout_rate , n_classes,length = 100):\n",
    "        super(Classifier , self).__init__()\n",
    "        self.t2v = Time2Vec(n_features)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(n_features*4,n_heads,hidden_size,dropout_rate),n_layers) # Transformer encoder layer \n",
    "        self.pooling= nn.AdaptiveAvgPool1d(1) # Global average pooling layer \n",
    "        self.linear= nn.Linear(n_features*4,n_classes) # Linear layer with softmax activation \n",
    "\n",
    "    def forward(self,x):\n",
    "        # x shape: (batch_size ,seq_len ,n_features)\n",
    "        x = self.t2v(x)\n",
    "        # print(x.shape)\n",
    "        x= self.transformer_encoder(x) # Apply transformer encoder on x \n",
    "        # print(x.shape)\n",
    "        x= x.permute(0 ,2 ,1) # Permute x to match expected shape for pooling (batch_size ,n_features ,seq_len)\n",
    "        x= self.pooling(x) # Apply pooling on x \n",
    "        x= x.squeeze(-1) # Remove last dimension \n",
    "        x= self.linear(x) # Apply linear layer on x \n",
    "        return nn.functional.softmax(x,dim=-1) #Return class probabilities (batch_size ,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(\n",
    "    n_layers=n_layers,\n",
    "    n_features=n_features,\n",
    "    n_heads=n_heads,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout_rate=dropout_rate,\n",
    "    n_classes=n_classes,\n",
    "    length=4680,\n",
    ").to(\"cuda:0\")\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor(\n",
    "        [\n",
    "            0.9995,\n",
    "            1.0000,\n",
    "            0.4342,\n",
    "            1.0000,\n",
    "            0.9999,\n",
    "            0.9582,\n",
    "            0.9980,\n",
    "            1.0000,\n",
    "            0.9582,\n",
    "            1.0000,\n",
    "            0.3286,\n",
    "            0.9984,\n",
    "            0.9999,\n",
    "            0.9882,\n",
    "            0.2075,\n",
    "            0.9975,\n",
    "            0.1007,\n",
    "            1.0000,\n",
    "            0.9923,\n",
    "        ]\n",
    "    )\n",
    ").to(\n",
    "    \"cuda:0\"\n",
    ")  # Cross entropy loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-3)  # Adam optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "# Train the model on the training data \n",
    "for epoch in range(n_epochs): #Iterate over epochs \n",
    "    epochloss = 0\n",
    "    ldl = len(train_dl)\n",
    "    for bnum,(batch_x,batch_y) in enumerate(train_dl): #Iterate over batches \n",
    "        # print(batch_x.shape,batch_y.shape)\n",
    "        batch_x = batch_x.to(\"cuda:0\").to(torch.float)\n",
    "        batch_y = batch_y.to(\"cuda:0\").to(torch.long)[:,0]\n",
    "        optimizer.zero_grad() #Clear previous gradients \n",
    "        \n",
    "        output=model(batch_x).to(torch.float) #Get model output for current batch (batch_step ,n_classes)\n",
    "        # print(output.shape)\n",
    "        loss=loss_fn(output.to(torch.float),batch_y) #Compute loss for current batch \n",
    "        loss.backward() #Backpropagate loss \n",
    "        \n",
    "        optimizer.step() #Update parameters \n",
    "        \n",
    "        print(f\"Epoch {epoch} Batch {bnum}/{ldl} : Loss {loss.item()}\",end = \"\\r\",flush=True) #Print epoch and loss \n",
    "        epochloss+=loss.item()\n",
    "    if epoch > 20 and epoch < 300: scheduler.step() \n",
    "    epochloss/=ldl\n",
    "\n",
    "    print(f\"\\nAverage epoch loss: {epochloss} with Learning rate {scheduler.get_last_lr()}\")\n",
    "    print(\"Evaluating with\",end = \" \")\n",
    "    with torch.no_grad(): # Disable gradient computation \n",
    "        accsum = []\n",
    "        for bnum,(batch_x,batch_y) in enumerate(train_dl):\n",
    "            batch_x = batch_x.to(\"cuda:0\")\n",
    "            batch_y = batch_y.to(\"cuda:0\").to(torch.long)[:,0]\n",
    "            output=model(batch_x) #Get model output for validation data (val_batch_size ,n_classes)\n",
    "            pred=torch.argmax(output,dim=-1) # Get predicted classes (val_batch_size)\n",
    "            accsum.append((pred==batch_y).float()) # Compute accuracy (%)\n",
    "            # print(inp.shape, output.shape, pred.shape, rout.shape,acc)\n",
    "        # print(f\"Accuracy on validation data : {acc}%\")# Print accuracy\n",
    "        equals= torch.cat(accsum)\n",
    "        means = torch.mean(equals)\n",
    "        print(\"mean total accuracy:\", means.item()*100,\"%\\n\",\"-=<{|}>=-\"*8,'\\n')\n",
    "    # Evaluate the model on the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # Disable gradient computation \n",
    "    accsum = []\n",
    "    for bnum,(batch_x,batch_y) in enumerate(test_dl):\n",
    "        batch_x = batch_x.to(\"cuda:0\")\n",
    "        batch_y = batch_y.to(\"cuda:0\")\n",
    "        print(bnum)\n",
    "        output=model(batch_x) #Get model output for validation data (val_batch_size ,n_classes)\n",
    "        pred=torch.argmax(output,dim=-1) # Get predicted classes (val_batch_size)\n",
    "        accsum.append((pred==batch_y).float()) # Compute accuracy (%)\n",
    "        # print(inp.shape, output.shape, pred.shape, rout.shape,acc)\n",
    "    # print(f\"Accuracy on validation data : {acc}%\")# Print accuracy\n",
    "    equals= torch.cat(accsum)\n",
    "    means = torch.mean(equals)\n",
    "    print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=500_000)\n",
    "print(equals)\n",
    "torch.set_printoptions(profile='default')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
