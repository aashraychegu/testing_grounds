{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.k = k # Dimension of time2vec vector\n",
    "        self.w = nn.Parameter(torch.randn(k)) # Learnable parameters for linear part\n",
    "        self.b = nn.Parameter(torch.randn(k)) # Learnable parameters for linear part\n",
    "        self.w_sin = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "        self.b_sin = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "        self.w_cos = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "        self.b_cos = nn.Parameter(torch.randn(k)) # Learnable parameters for periodic part\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1) # Add extra dimension for vectorization\n",
    "        linear = self.w * x + self.b # Linear transformation (k features)\n",
    "        sin_trans = torch.sin(self.w_sin * x + self.b_sin) # Periodic transformation (k features)\n",
    "        cos_trans = torch.cos(self.w_cos * x + self.b_cos) # Periodic transformation (k features)\n",
    "        return torch.cat([linear, sin_trans, cos_trans], -1) # Concatenate along last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.0008363328694443018 0.0008363328694443018 0.0 0\n",
      "\n",
      "0.00028057973042173843 1.0002805797304206 1.0 1\n",
      "\n",
      "-0.0012012589032809732 1.9987987410967183 2.0 2\n",
      "\n",
      "-9.272215855407339e-05 2.999907277841447 3.0 3\n",
      "\n",
      "0.0008383334331666662 4.000838333433165 4.0 4\n",
      "\n",
      "0.000314786672998356 5.000314786672999 5.0 5\n",
      "\n",
      "0.001342008886545417 6.001342008886546 6.0 6\n",
      "\n",
      "0.0002904069749306065 7.000290406974934 7.0 7\n",
      "\n",
      "0.0005950789711064216 8.000595078971102 8.0 8\n",
      "\n",
      "0.0005970212269094795 9.00059702122691 9.0 9\n",
      "\n",
      "5.848621977691566e-06 10.000005848621973 10.0 10\n",
      "\n",
      "-9.752237062478575e-05 10.999902477629375 11.0 11\n",
      "\n",
      "-0.00023057688362219508 11.999769423116371 12.0 12\n",
      "\n",
      "-0.00043008894132254456 12.99956991105868 13.0 13\n",
      "\n",
      "-0.001177259016758164 13.99882274098325 14.0 14\n",
      "\n",
      "-9.294548892638851e-05 14.999907054511073 15.0 15\n",
      "\n",
      "0.001507606353922871 16.001507606353925 16.0 16\n",
      "\n",
      "0.0012731548423445455 17.001273154842345 17.0 17\n",
      "\n",
      "0.0007398046807101274 18.000739804680702 18.0 18\n",
      "\n",
      "0.0006868269015479005 19.000686826901553 19.0 19\n",
      "\n",
      "-0.0010448219604578485 -0.0010448219604578485 0.0 0\n",
      "\n",
      "-0.003040330042838299 0.9969596699571617 1.0 1\n",
      "\n",
      "0.002253405653932365 2.0022534056539323 2.0 2\n",
      "\n",
      "0.002165658104879206 3.002165658104879 3.0 3\n",
      "\n",
      "-0.004000217216481839 3.995999782783518 4.0 4\n",
      "\n",
      "-0.0016658494743757876 4.998334150525625 5.0 5\n",
      "\n",
      "-0.0018934531017834252 5.998106546898216 6.0 6\n",
      "\n",
      "0.0016770524744779932 7.001677052474478 7.0 7\n",
      "\n",
      "-0.005747059192686666 7.994252940807312 8.0 8\n",
      "\n",
      "-0.0009119490113873957 8.999088050988613 9.0 9\n",
      "\n",
      "-0.006417259051347394 9.993582740948654 10.0 10\n",
      "\n",
      "0.0027077454608225504 11.002707745460821 11.0 11\n",
      "\n",
      "0.00029954685372190153 12.000299546853723 12.0 12\n",
      "\n",
      "-0.002132206482773985 12.997867793517225 13.0 13\n",
      "\n",
      "0.0009589200613397981 14.000958920061342 14.0 14\n",
      "\n",
      "0.0014360584061162837 15.001436058406117 15.0 15\n",
      "\n",
      "-0.00016668174066006942 15.99983331825934 16.0 16\n",
      "\n",
      "0.0001424040538761612 17.000142404053875 17.0 17\n",
      "\n",
      "0.002200562176579949 18.00220056217658 18.0 18\n",
      "\n",
      "0.0036253570049950634 19.003625357004996 19.0 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 32 # Number of samples per batch\n",
    "seq_len = 100 # Length of each time series\n",
    "in_features = 1\n",
    "n_classes = 20 # Number of classes to predict\n",
    "\n",
    "# Create synthetic dataset (X: input features, y: labels)\n",
    "# X = np.random.randn(100,batch_size * seq_len * in_features).reshape(100, batch_size , seq_len)\n",
    "# y = np.random.randint(0, n_classes , size=(100,batch_size))\n",
    "def gen_ds(samples = 32_000,name = \"dl\",batch_size = 32):\n",
    "    assert samples%n_classes == 0\n",
    "    xcont = []\n",
    "    ycont = []\n",
    "    for i in range(n_classes):\n",
    "        cnumb = int(samples/n_classes)\n",
    "        ox = np.random.randn(samples,seq_len*in_features).reshape(samples,seq_len)\n",
    "        lx = ox + i\n",
    "        ly = np.full((samples),i)\n",
    "        print()\n",
    "        xcont.append(lx)\n",
    "        ycont.append(ly)\n",
    "        print(np.mean(ox),np.mean(lx),np.mean(ly),i)\n",
    "\n",
    "\n",
    "    X = np.concatenate(xcont)\n",
    "    Y = np.concatenate(ycont)\n",
    "\n",
    "    # Convert numpy arrays to tensors \n",
    "    X = torch.from_numpy(X).float()\n",
    "    y = torch.from_numpy(Y).long()\n",
    "\n",
    "    class tds(Dataset):\n",
    "        def __init__(self,x,y,name) -> None:\n",
    "            super().__init__()\n",
    "            self.name = name\n",
    "            self.X = x\n",
    "            self.Y = y\n",
    "            self.pspace = len(self.X)\n",
    "        def __len__(self):\n",
    "            return self.pspace\n",
    "        def __getitem__(self, index):\n",
    "            return X[index],Y[index]\n",
    "    ds = tds(X,y,name)\n",
    "    dl = DataLoader(ds,batch_size=batch_size,shuffle=True)\n",
    "    return dl\n",
    "train_dl = gen_ds(samples = 800*20,name = \"train_dl\")\n",
    "test_dl = gen_ds(1000,name = \"test_dl\",batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "n_features = 16 # Number of features per time step\n",
    "n_layers = 4 # Number of transformer encoder layers\n",
    "n_heads = 16 # Number of attention heads per layer\n",
    "hidden_size = 2048 # Size of hidden state in sublayers \n",
    "dropout_rate = 0.2 # Dropout rate for regularization\n",
    "\n",
    "\n",
    "# Define classifier model \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self , n_layers , n_features , n_heads , hidden_size , dropout_rate , n_classes,length = 100):\n",
    "        super(Classifier , self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(length)\n",
    "        self.t2v = Time2Vec(n_features)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(n_features*3,n_heads,hidden_size,dropout_rate),n_layers) # Transformer encoder layer \n",
    "        self.pooling= nn.AdaptiveAvgPool1d(1) # Global average pooling layer \n",
    "        self.linear= nn.Linear(n_features*3,n_classes) # Linear layer with softmax activation \n",
    "\n",
    "    def forward(self,x):\n",
    "        # x shape: (batch_size ,seq_len ,n_features)\n",
    "        x = self.batch_norm(x).to(\"cuda:0\")\n",
    "        # print(x.shape)\n",
    "        x = self.t2v(x)\n",
    "        # print(x.shape)\n",
    "        x= self.transformer_encoder(x) # Apply transformer encoder on x \n",
    "        # print(x.shape)\n",
    "        x= x.permute(0 ,2 ,1) # Permute x to match expected shape for pooling (batch_size ,n_features ,seq_len)\n",
    "        x= self.pooling(x) # Apply pooling on x \n",
    "        x= x.squeeze(-1) # Remove last dimension \n",
    "        x= self.linear(x) # Apply linear layer on x \n",
    "        return nn.functional.softmax(x,dim=-1) #Return class probabilities (batch_size ,n_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Source: Conversation with Bing, 17/02/2023(1) Transformers for Time Series â€” Transformer 0.3.0 documentation. https://timeseriestransformer.readthedocs.io/en/latest/README.html Accessed 17/02/2023.\n",
    "# (2) Transformer Time Series Prediction - GitHub. https://github.com/oliverguhr/transformer-time-series-prediction Accessed 17/02/2023.\n",
    "# (3) Transformers for Time Series - GitHub. https://github.com/maxjcohen/transformer Accessed 17/02/2023.\n",
    "# (4) GitHub - gzerveas/mvts_transformer: Multivariate Time Series .... https://github.com/gzerveas/mvts_transformer Accessed 17/02/2023.\n",
    "# (5) Timeseries classification with a Transformer model - Keras. https://keras.io/examples/timeseries/timeseries_transformer_classification/ Accessed 17/02/2023. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Classifier(n_layers=n_layers,n_features = n_features,n_heads=n_heads,hidden_size=hidden_size,dropout_rate=dropout_rate,n_classes=n_classes,length=100).to(\"cuda:0\")\n",
    "# Define loss function and optimizer \n",
    "loss_fn= nn.CrossEntropyLoss().to(\"cuda:0\") # Cross entropy loss function \n",
    "optimizer= optim.AdamW(model.parameters(),lr = 1e-5) #Adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 9999/10000 : Loss 2.0882122516632083\n",
      "Epoch loss: 2.3003254974603653\n",
      "Evaluating with mean total accuracy: 0.7921093702316284 \n",
      " -=<{|}>=--=<{|}>=--=<{|}>=--=<{|}>=--=<{|}>=--=<{|}>=--=<{|}>=--=<{|}>=-\n",
      "Epoch 1 Batch 906/10000 : Loss 2.2379522323608476\r"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "# Train the model on the training data \n",
    "for epoch in range(n_epochs): #Iterate over epochs \n",
    "    epochloss = 0\n",
    "    ldl = len(train_dl)\n",
    "    for bnum,(batch_x,batch_y) in enumerate(train_dl): #Iterate over batches \n",
    "        # print(batch_x.shape,batch_y.shape)\n",
    "        batch_x = batch_x.to(\"cuda:0\")\n",
    "        batch_y = batch_y.to(\"cuda:0\").to(torch.long)\n",
    "\n",
    "        optimizer.zero_grad() #Clear previous gradients \n",
    "        \n",
    "        output=model(batch_x) #Get model output for current batch (batch_step ,n_classes)\n",
    "        # print(output.shape)\n",
    "        loss=loss_fn(output,batch_y) #Compute loss for current batch \n",
    "        loss.backward() #Backpropagate loss \n",
    "        \n",
    "        optimizer.step() #Update parameters \n",
    "        \n",
    "        print(f\"Epoch {epoch} Batch {bnum}/{ldl} : Loss {loss.item()}\",end = \"\\r\",flush=True) #Print epoch and loss \n",
    "        epochloss+=loss.item()\n",
    "    epochloss/=(bnum+1)\n",
    "\n",
    "    print(f\"\\nEpoch loss: {epochloss}\")\n",
    "    print(\"Evaluating with\",end = \" \")\n",
    "    with torch.no_grad(): # Disable gradient computation \n",
    "        accsum = []\n",
    "        for bnum,(batch_x,batch_y) in enumerate(train_dl):\n",
    "            batch_x = batch_x.to(\"cuda:0\")\n",
    "            batch_y = batch_y.to(\"cuda:0\").to(torch.long)\n",
    "            output=model(batch_x) #Get model output for validation data (val_batch_size ,n_classes)\n",
    "            pred=torch.argmax(output,dim=-1) # Get predicted classes (val_batch_size)\n",
    "            accsum.append((pred==batch_y).float()) # Compute accuracy (%)\n",
    "            # print(inp.shape, output.shape, pred.shape, rout.shape,acc)\n",
    "        # print(f\"Accuracy on validation data : {acc}%\")# Print accuracy\n",
    "        equals= torch.cat(accsum)\n",
    "        means = torch.mean(equals)\n",
    "        print(\"mean total accuracy:\", means.item(),\"\\n\",\"-=<{|}>=-\"*8)\n",
    "    # Evaluate the model on the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # Disable gradient computation \n",
    "    accsum = []\n",
    "    for bnum,(batch_x,batch_y) in enumerate(test_dl):\n",
    "        batch_x = batch_x.to(\"cuda:0\")\n",
    "        batch_y = batch_y.to(\"cuda:0\")\n",
    "        print(bnum)\n",
    "        output=model(batch_x) #Get model output for validation data (val_batch_size ,n_classes)\n",
    "        pred=torch.argmax(output,dim=-1) # Get predicted classes (val_batch_size)\n",
    "        accsum.append((pred==batch_y).float()) # Compute accuracy (%)\n",
    "        # print(inp.shape, output.shape, pred.shape, rout.shape,acc)\n",
    "    # print(f\"Accuracy on validation data : {acc}%\")# Print accuracy\n",
    "    equals= torch.cat(accsum)\n",
    "    means = torch.mean(equals)\n",
    "    print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=500_000)\n",
    "print(equals)\n",
    "torch.set_printoptions(profile='default')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
